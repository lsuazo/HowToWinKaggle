\documentclass[a4paper]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{xcolor}
\usepackage{amsmath}		

\newcommand{\brown}{\color{brown}}
\renewcommand{\t}{\hspace*{0.5cm}}


\newenvironment{tightcenter}{
  \setlength\topsep{0pt}
  \setlength\parskip{0pt}
  \begin{center}
  }{
  \end{center}
}

\newenvironment{codesnip}[1]
{\begin{tightcenter}\begin{minipage}{.85\textwidth}#1}
{\end{minipage}\end{tightcenter}}

\title{How to Win a Data Science Competition: Lean From Top Kagglers}
\author{Coursera - National Research University Higher School of Economics}
\date{2020}

\begin{document}
\maketitle

\chapter{week1}
\section{Introduction and Recap}
\begin{itemize}
  \item In Kaggle competitions, looks for {\bf Kernels} and/or {\bf Notebooks} - people share code that way and you can learn from them.
  \item IMPORTANT: \underline{Insight is more important than algorithm}
  \item IMPORTANT: Don't limit yourself - use advanced feature enginnering; run huge greedy calculations over night.
  \item Recap of models/methods:
    \begin{itemize}
      \item Linear Methods
	\begin{itemize}
	  \item Logistic Regression; Support Vector Machine
	  \item GOOD FOR: Sparse, high dimensional data
	  \item BAD: Often linear separability is not a good approximation
	\end{itemize}
      \item Tree Based Methods
	\begin{itemize}
	  \item Decision Tree; Random Forrest; Gradient Boosted Decision Trees (GBDT)
	  \item GOOD FOR: good default method for tabular data; good for non-linear relationships
	  \item BAD FOR: hard to capture linear separation (say diagonal line in a 2D plane)
	    \subitem from sklearn: can create over-complex trees that do not generalise data well - that is they are prone to overfitting
	    \subitem trees can't extrapolate trends - they are good at interpolation not extrapolation
	  \item \emph{Sklearn} has good random forrest; \emph{xgboost} has good GBDT
	\end{itemize}
      \item K-Nearest neighboors (KNN)
	\subitem good for feature building
      \item Neural Networks (NN)
	\subitem good for images, sounds, texts and sequences
	\subitem very good framework is \emph{pytorch}
    \end{itemize}
  \item Most powerfull methods currently are GBDS and NN
  \item \underline{No free lunch theorem} - there is no single methods that beats all other methods for all applications
  \item TODO: Look into H20 - open source, in-memory, distributed, fast and scalable machine learning and predictive analytics platform.
    \subitem Looks quite interesting!
  \item Hardware: SSD is critical; use cloud computing for anything too big.
\end{itemize}	

\section{Feature Preprocessing}
\begin{itemize}
  \item VIP: Strong connection between preprocessing and model choice
  \item Scaling: $ x_i \to \alpha x_i$
    \begin{itemize}
      \item decision trees are not affected by a scaling tx, but non-tree based methods like NN, kNN and regularized linear models are very affected
	\subitem in KNN - $x \to 0*x$ means ignore $x$, while $x \to \infty x$ means x dominates
	\subitem Any methods that relies on gradient descent is sensitive to scaling (logistic regression, SVM, neural networks, etc. etc)
      \item appraoch 1: Normalization (map to [0,1])
	$$ x \to (x - x.min())/(x.max() - x.min) $$
        \subitem code: {\color{brown} sklearn.preprocessing.MinMaxSaler}
      \item approach 2: standardization (to $\mu=0, \sigma=1$)
	$$ x \to (x - x.mean())/x.std()$$
	\subitem code: {\brown sklearn.preprocessing.StandardScaler} 
      \item Can use scaling to boost or attenuate signals - so scaling can be thought as just another hyperparameter to tune
	\subitem In general, start with all features in the same scale and explore changing that.
    \end{itemize}
  \item Outliers
    \begin{itemize}
      \item linear models extremely sensitive
      \item approach 1: {\bf Winsorization} - clip values between percentiles
	\subitem code: {\brown UPPER, LOWER = np.percentile(x, [1,99]); y = np.clip(x, UPPER, LOWER)}
	\subitem very common in finance
      \item approach 3: Rank
	\subitem good for knn and linear models when you don't have time to handle outliers by hand
      \item approach 3: Other transformation
	\subitem log transformation; raising to power < 1 - these have the benefit of bringing outliers in closer, and spreading things around zero apart
    \end{itemize}
  \item Missing values
    \begin{itemize}
      \item can be Nans, empty strings, outliers like -1, or -999
      \item identifiying when a value actually represents a missing value can be challenging. Main tool: Histogram.
      \item Sometimes missing values contain alot of useful info - there might be a reason the value is missing
	\subitem So it is good to create new features like 'isMissing' or something like that.
      \item 3 main imputation techniques
	\begin{itemize}
	  \item NaN $\to$ value outside range
	    \subitem Gives trees possibilities to use this; Performance for linear models suffers greatly
	    \subitem Curated data often comes with somethign like this already (as described above)
	  \item NaN $\to$ mean, median or mode.
	    \subitem good for linear methods; not good for trees
	  \item Try to reconstruct NaN
	    \subitem not easy; need to know something about the data generating process.
	\end{itemize}
      \item WARNING: \underline{be vary careful with early NaN imputation if you then build features based on imputed feature}
	\subitem for exampl if you fill a cyclic feature with median values, and then construct diff as new feature - will lead to prominent discontinuities and flat zones.
      \item some libraries, like XGBoost can handle NaNs automatically

    \end{itemize}
  \item For all all of these, you must learn the transformation from training data, and then apply the same one on testing. Sklearn Transformation API (used by the transformations above) allows you to do this. VERY GOOD.
  \item USEFUL TRICK: create different predictors from same feature using different preprocessing techniques. 
  \item USEFUL TRICK: Mix models that are trained on different preprocessed versions of the data
\end{itemize}

\section{External reading}
\begin{itemize}
  \item FROM SKLEARN
    \begin{itemize}
      \item when data is sparse, you don't want a scaler that moves zeros. Think about nature of your data to chose scalar.
      \item Transfromer API: {\brown scaler = sklearn.Preprocessing.StandardScaler().fit(X\_train); x\_test\_scaled = scaler(X\_test)}
      \item many estimators in sklearn expect features to look more or less $\sim$ N(0,1)
      \item VIP: Note l1 and l2 regularizes assume all features centered around zero and have variance in the same order.
      \item other useful tx:
	\begin{itemize}
	  \item preprocessing.Normalizer() - scaling individual samples to have unit norm (useful when estimating pair similarity via dot prod)
	  \item preprocessing.OneHotEncoder(), preprocessing.OrdinalEncoder()
	  \item preprocessing.KBinDiscretizer(n\_bins, encode).fit() - different strategies available
	    \subitem histograms focus on counting features in particular bin, whereas dsicretizers focus on labeling features with bin
	\end{itemize}
    \end{itemize}
  \item Sebastian Raschka
    \begin{itemize}
      \item Tree based classification is probably the only scale invariant algo
      \item VIP: when using PCA, it is better to standardize (mean 1, std 0) than just normalize (map to [0,1]) - scaling affects covariance
	\subitem cool example doing PCA and then bayes classifier on top - compar prediction score.
    \end{itemize}
  \item Jason Brownlee - Discover feature engineering: ``Most algorithms are standard - we spend most of our efforts on feature engineering''
  \item Rahul Agarwal - Best practices in feature engineering
    \begin{itemize}
      \item LogLoss clipping technique: clip prediction prob to [0.05. 0.95]  when using log loss metric - it penalizes heavily being very certain and wrong
      \item Use PCA for rotation, not only dim rec
      \item sometimes add interaction features, A*B, A/B, A+B, A - B
    \end{itemize}
\end{itemize}

\section{Feature Generation}
\begin{itemize}
  \item encodings categorical/ordinal
    \begin{itemize}
      \item label encoding: map categories to 1,2,3,4\ldots
	\subitem good for tree based methods, not good for knn or linear (because it assume order and proportionality in labels - moreover dependence is likely not-linear)
	\subitem code: {\brown sklearn.preprocessing.LabelEncoder()} or {\brown pd.factorize()}
      \item Frequency encoding: map to numerical value representing frquency of category
	\subitem can help trees use less splits
	\subitem this is even sometimes useful in linear models
      \item one-hot encoding: create indicator variable for each category
	\subitem these can be good for linear methods, but in general slow down methods (explosion of features) and might not help (specially trees)
	\subitem though will help tree if target depends on lbael encoded feature in a very non linear way
	\subitem code: {\brown pd.get\_dummies()} or {\brown sklearn.preprocessing.OneHotEncoder()}
    \end{itemize}
  \item Datetime and coordinates
    \begin{itemize}
      \item Very different than simple numerical or categorical because we can interpret their meaning - they have much context
      \item dates and times can lead to two  main types of features
	\begin{itemize}
	  \item moments in a period (ie using periodicity of datetime)
	    \subitem ex: day of week, day of month, month in year, etc.  Or minute value, hour value, etc.
	    \subitem useful to capture repetitive pattern in data
	  \item time since/to event
	    \subitem can be row independent: years since 2000, for all rows (so all rows have same reference)
	    \subitem row dependent: days since last holiday (or till next holiday) - two rows can be referring to different holidays
	\end{itemize}
      \item very useful to diffs between two date columns
      \item once you generate new features, numerical or categorical, preprocess them accordingly
      \item coordinates
	\begin{itemize}
	  \item typically you want to calculate distance to points of interest (nearest hospital, school, etc) 
	  \item very useful to calculate aggregated statistics for objects around an area
	    \subitem ex: \# of flats around a point -> proxy for popularity of area
	    \subitem ex: mean price of flats around a point -> gives sense of price of area.
	\end{itemize}
    \end{itemize}
  \item Collection of tricks
    \begin{itemize}
      \item separte price into integer part and fractional part - can utilize differences in peoples \emph{perception} of a price
      \item Create feature, 'isRoundNumber' - people often use numers like 5 and 10, while robots can use many decimals.
      \item One-hot encode interaction between categorical features [just concatenate strings and OHE result]
	\subitem Not so useful in tree based models, because they can easily approximate this with individual categories.
      \item Sometimes simple multiplication, or division of features makes a huge difference.	
	\subitem linear models can't approximate these, and trees have a very hard time approximating them.
      \item sometimes useful to add slightly rotated coordinate system - particularly when using trees. 
	\subitem ex: if a particular street happens to be a good division, but that street is not algined with coordinates, then tree uses many split to approximate.
	\subitem hard to know what rotation to use a priori - so add several and check effect.
    \end{itemize}
\end{itemize}

\section{Feature Extraction From Texts and Images}
\begin{itemize}
  \item For Text\ldots
  \item Preprocessing: 1) lowercase, 2) lemmatization, 3) stemming, 4) remove stopwords
  \item feature extraction: Bag of words - column per word in corpus, row perd doc, count ocurrences
	\subitem can extend to n-grams, of either words of letters
   \item Post processing: TFiDF (Term Frequency and Inverse document frequency)
   \item OR \ldots
   \item embeddings (word2vec, or others)
     \begin{itemize}
       \item Still use preprocessing
       \item create vector representation of words in text
       \item uses nearby words (unsupervised)
       \item often resulting vector space has interpretable operations
       \item training can take a long time - check for pretrained
     \end{itemize}
   \item BOW and Word2Vec often give very different results - use bott together
   \item for images 
     \begin{itemize}
       \item look for pretained models and do some fine tunning
       \item use image augmentation to increase training samples (crops, rotations, adding noise, etc.)
	 \subitem reduces overfitting
     \end{itemize}
\end{itemize}

\section{Questions}
\begin{itemize}
  \item	in GBM\_drop\_tree notebook - why are raw predictions - the output of the staged\_decision\_function - approaching $\pm 8$? (I assume it has to do with depth 3 choice of trees, but still, shouldn't output be close to $\pm1$ which is actual y-values?
\end{itemize}

\chapter{Week 2}
\section{EDA}
\begin{itemize}
  \item VIP: \underline{\bf EDA is key}
    \subitem Kaggle CEO says two apporaches: 1) is EDA other is 2) deep NN and pass everything. They have different domains in which they work better.
  \item Steps:
    \begin{enumerate}
      \item Get Domain knowledge - google, read wiki, read state of the art (put in the time!)
      \item Check whether values in data set agree with domain knowloedge - var ranges, typos, etc. (systematic errors can be very useful info)
      \item KEY: figure out the generation process used in the data - must try to mimic to set up proper validation scheme.
	\subitem perhaps they did random sample; perhaps they oversampled a class to balance out; some times train and test set are produce very differently. INVESTIGATE
    \end{enumerate}
  \item Exploring Anonimized data
    \begin{itemize}
      \item anonimized data is data which orgranizers intentionally change so as to not reveal some information  (ex: replace words with hash values, or col names with x1, x2 ..)
      \item Things to try:
	\begin{itemize}
	  \item try to decode (very diffiicult, and most of the time, can't do it)
	  \item guess the meaning of the column 
	    \subitem cool example was trying to unstardadize a numerical column, by reverse engineering the mean and std dev, to find out original column was year born
	  \item guess the type: numeric, categorical, etc.   - this is generally doable
	  \item find out how feature relate to each other - find pairwise relationships (scatter plots) or even groups (correlatio plots, etc)
	\end{itemize}
      \item tip: label encode using pandas factorize (encodes based on order of appearance)
	\subitem {\color{brown} for c in train.columns[train.dtypes ==  'object']: x[c] = x[c].factorize()[0]}
      \item \underline{\bf VIP TIP}: fit a random forrest and then use {\color{brown} plt.plot(rf.feature\_importances\_} - this can tell you which features you should work on most.
    \end{itemize}
  \item VIP: \underline{Linear models can eaily extract sums and differences, but tree based methods cannot!!}
\end{itemize}	

\section{visualizations}
\begin{itemize}
  \item EDA is an art, and visualizations are our tools
  \item Plots to explore individual features
  \item VIP: \underline{Never make a conlusion from a single plot!} if you have a hypothesis, try to come up with several different plots that could disprove it!
    \begin{itemize}
      \item {\color{brown} plt.hist}
	\subitem can be misleading - vary bin nums. Also zoom in.
	\subitem if see alot of thinks like 12, 24, 26 - i.e. separated by same amount then generate feature x \% 12 (or whatever the appropriate number)
      \item {\color{brown} plt.plot(x,'.')}
	\subitem convenient not to connect points with line segments - just use dots
	\subitem if horizontal lines appear, then lots of repeated values - IN THIS CASE, CREATE FEATURE THAT COUNTS HOW MANY COLS SAME IN THE GROUP - or feature for more nuanced pattern in group.
	\subitem if vertical patterns, then data is not shuffled - IN THIS CASE ADD ROW INDEX AS FEATURE
      \item {\color{brown} plt.scatter(range(len(x), x, c=y)} - very good for looking for separation in the classes based on that feature.
    \end{itemize}
  \item Explore feature relationships
    \begin{itemize}
      \item {\color{brown} plt.scatter(x1, x1)} - \underline{One of the best tools!}
	\subitem usually color by class label too
	\subitem for regression used heaptmap type colors, or visualize target value as point size
  	\subitem TIP: useuful to overlap colored train data with uncolored test data 
	\subitem scatter plots can lead to finding mathematical relations between features (like $x1 \leq x2$) - use these to generate new features like diff or ratio
	\subitem if small number of features: {\color{brown} pd.scatter\_matrix(df)}
      \item large scale feature similarity: {\color{brown} df.corr()} and {\color{brown} plt.matshow()}
	\subitem instead of corr, try to create matrices like: how many times in one feature greater than the other (this can spot cummulative features), or how many distinct combinations of these two features in data exist (can spot relationships between labels)
	\subitem really  cool algorithm for grouping based on these corr values: {\bf spectral biclustering algorithm} - {\color{red}LOOK INTO THIS!}
      \item {\color{brown} df.mean().plot(style='.') }
	\subitem particularly if you sort the columns based on that statistic - might see groups
      \item AND MANY MORE - BE CREATIVE
    \end{itemize}
  \item if you find groups of related features, it is often good to generate new features like a mean value of the group, etc.
  \item nice code when overlapping values:\\
    \begin{codesnip}{\brown}
    def jitter(data, stdev):\\
    \hspace*{1cm} N  = len(data)\\
    \hspace*{1cm} return data + np.random.rand(N)*stdev\\
	plt.scatter(jitter(x, sigma), jitter(y, sigma), c=y)
      \end{codesnip}
\end{itemize}

\section{Dataset cleaning and things to check}
\begin{itemize}
  \item Find (and discard) features that are constant in train and test
    \subitem {\color{brown} traintest.nunique(axis =1) == 1}
    \subitem if constant in training and non-constant in test, still remove
  \item find and remove duplicate features
    \subitem {\color{brown} traintest.T.drop\_duplicates()}
    \subitem if duplicate categorical but with diff cat names label encode first, using factorize()!!\\
  for f in categorical\_feats:\\
  \hspace*{1cm} train[f] = traintest[f].factorize()\\
  traintest.T.drop\_duplicates()
  \item check for duplicate rows
    \subitem check if duplicate rows have diff targets - if so, understand why!
  \item check if train and test have common rows - if so, why?
  \item check if data is shuffled (plot feature or target vs row index)
    \subitem if not shuffled, high chace leakage exists
  \item Nice code - feature histograms
    \begin{codesnip}{\brown}
      def hist\_it(feat): \\
      \t plt.figure(figsize=(16,4))\\
      \t feat[Y == 0].hist(bins=range(int(feat.min()), int(feat.max()+2)), normed=True, alpha=0.8)
      \t feat[Y == 1].hist(bins=range(int(feat.min()), int(feat.max()+2)), normed=True, alpha=0.8)
      \t plt.ylim((0,1))

    \end{codesnip}
  \item Nice code - compare categoricals
    \begin{codesnip}{\brown}
      train\_enc = pd.DataFrame(index = train.index)\\
      for col in tqdm\_notebook(train.columns):\\
      \t train\_enc[col] = train[col].factorize()[0] \\

      dup\_cols = \{  \}
      for i, c1 in enumerate(tqdm\_notebook(train\_enc.columns)):\\
      \t for c2 in train\_enc.columns[i+1: ]:\\
      \t\t if c2 not in dup\_cols and np.all(train\_enc[c1] ==  train\_enc[c2]):
      \t\t\t dup\_cols[c2] == c1
    \end{codesnip}
  \item Usually convenient to concatenate train and test, and do all feature engineering with result (but not always)
\end{itemize}

\section{Validation}
\begin{itemize}
  \item Validation is a piece of test data not used for fitting, but rather checking the value of the model over unseen data. 
  \item Overffiting in general != Overfitting in competitions
    \subitem former: train quality > test quality
    \subitem latter: when quality in test is worse than expected
  \item MAIN POINT: have train/validation split mimic the train/test split - THIS IS KEY. 
    \subitem without this, you validation score does not represent your out of sample test score.
    \subitem Sometimes it can be very challenging to figure out how train/test split was made. It is worth spending considerable time here if needed.
  \item validation strategies
    \begin{itemize}
      \item  3 main: Holdout, K-fold, and leave one out.
      \item holdout - usually a good choice when there is enough data
	\subitem no overlap between train and validation
      \item k-fold - basically repeated holdout, where entire data is partitioned into k validation sets - for each validation set, model is trained on complement. Final measure of performance is average over k folds.
	\subitem core idea: every sample is used for validation exactly once
	\subitem usually k=5 is a good starting point
      \item LOO - is k-fold where k = len(train)
	\subitem used only when there is very little data
    \end{itemize}
  \item usually houldout and k-fold on shuffled data not good when:
    \begin{itemize}
      \item unbalanced data sets (as far as classes)
      \item multiclass classification with many classes
    \end{itemize}
  \item in these cases use {\bf stratification} - preserve the same target distribution in the different folds
  \item these methods are also generally not good for time series data
    \begin{itemize}
      \item you generally need to make a time split - so if holdout, don't shuffle.
	\subitem this emulates how time series data is received, and used
      \item the time series version of k-fold, is a \emph{moving window cross validation} - this rely on capturing trends
      \item OTOH, if for competiotn, test/train split did not use a time split, that means you have future data. Then USE it! And have your cv emulate (this is generally not the case)
      \item models/features that rely on trends tend to be very different than those that rely on future data, so it is key to get this right.
    \end{itemize}
  \item main types of splits used in competitions
    \begin{itemize}
      \item random split (rowwise) - done when rows are fairly independent of each other
      \item time based split
      \item by ID - (typically several rows per ID)
	\subitem test will have IDs not seen in train
      \item combined - ex: date split for each ID independently
      \item non-trivial
    \end{itemize}
  \item Typical problems encountered during validation
    \begin{itemize}
      \item Different folds lead to very different values
	\subitem can happen when different folds are very different in nature: example, in competition, cross-validating with january vs february can be very different (number of holidays is very different) {\color{red} HINT HINT}
	\subitem can also be caused by too little data, or data that is too diverse, or data that is inconsistent (i.e. similar samples with very different target values - error changes if they are both in train, or one and one)
      \item {\bf leaderboard shuffle} - very different public/ private scores.
      \item some solutions: increase k. Or redo k-fold with different seeds (can use one to get parameters and one to test)
    \end{itemize}
  \item submission stage problems: LB consistently higher (lower) than cv; LB uncorrelated with CV 
    \subitem can be caused by train/test data comminb from diff distributions -- Best friend EDA: problem is typically that you haven't mimicked split correctly
    \subitem Other trick: adjust solution based on LB - find optimal constant to add/subtract from your preductions based on public leaderboard score
    \subitem most often problem comes from imbalanced classes. solution, try to mimic the split.
    \subitem can also be casued by too little public test data; in this case just trust you CV
\end{itemize}

\section{Data leakages}
\begin{itemize}
  \item very bad, people get very sensitive, to exploit or not exploit? (shouldn't and often can't in real world)
    \subitem When they exist, they tend to dominate
  \item typical leaks
    \begin{itemize}
      \item future peaking [incorrect splits]
      \item metadata
      \item IDS sometimes contain information
      \item row ordering
      \item LB probing (can be used to extract what they call \emph{ground truth}
    \end{itemize}
\end{itemize}

\section{Additional Material And Links}
\begin{itemize}
  \item CV in sklearn
    \begin{itemize}
      \item Validation is a way to fit hyperparameters of model without burning data
      \item KEY: Preprocessing should also be learned from training, so it is good to use a {\bf pipeline} to combine prepreocessing with validation
	\subitem ex:
	\begin{codesnip}{\brown}
	  from sklearn.pipeline import make\_pipeline\\
	  clf = make\_pipeline(preprocessing.standardScaler(), svm.svc(c=1))\\
	  cross\_val\_score(clf, X, y, cv=cv)
	\end{codesnip}
      \item random split: {\brown sklearn.model\_selection.train\_test\_split}
      \item k-fold: {\brown sklearn.model\_selection.cross\_val\_score(df, x, y, cv=5, scoring='\ldots')}
	\subitem can pass a cross-validation scheme to cv to have more control, or even a custom iterable yielding (test, train) splits
	\subitem more flexible variant: cross\_validate
      \item many cross validation iterators that can be used, for iid and not iid (grouped, imbalanced classes, time series)
    \end{itemize}
  \item Some dude's lessons
    \begin{itemize}
      \item always cv (not just v)
      \item trust good CV method more than LB
      \item for final submission picke two very different models (ie. from bagging of SVM, vs Random forrest, vs neural network, vs linear models)
    \end{itemize}
  \item Kaggle CEO lessons learned
    \begin{itemize}
      \item Use sklearn pipelines to avoid leakages, peaking, etc.
      \item in 2011 random forrest won alot; in 2012 deep NN started dominating (at least vision and time series); in 2015 XGboost started dominating
      \item Two main approaches to dominate
	\begin{itemize}
	  \item XGBoost with serious EDA
	  \item deep NN with very little EDA
	\end{itemize}
      \item Top Kaggle participant attributes
	\begin{itemize}
	  \item creativity - create lots and lots of features
	  \item Tenacity - keep working, not get despirited
	  \item very good with statistics [avoid overfitting]
	  \item good software practices [like VC]
	\end{itemize}
      \item suggestion: look at kaggle public data sets, and use their script forking
    \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Week 3}
\section{Evaluation Metrics}
\begin{itemize}
  \item  Problem: Some metrics cannot be optimized efficiently. So need to come up with proxy metric, and find heuristics to transform optimized metric results to final submission.
  \item also, if train and test sets are different, might need to modify optimization metric a bit (happens often in time series, where distribution changes over time)
  \item interesting example metrix (comes from finance)
    \begin{equation}
      Loss(\hat{y}_i, y_i) = 
      \begin{cases}
	|y_i - \hat{y}_i| & \mbox{ if trend predicted correctly} \\
	(y_i - \hat{y}_i)^2 & \mbox{ if trend predicted incorrecly}
      \end{cases}
    \end{equation}
    \subitem punishes one type of error much more.
    \subitem Hard to optimize with an algorithm
    \subitem in his case, he hacked the metric - it was all about making small predictions, and guessing right size (or something like that)
  \item Do exploratory analysis of unusual metrics!
  \item ALWAYS BUILD A BEST CONSTANT BASELINE (see below)
\end{itemize}

\subsection{Regression metrics}
\begin{itemize}
  \item MSE := Mean Squared Error
    \begin{equation}
      MSE = \frac{1}{N}\sum_{i=1}^N(y_i - \hat{y}_i)^2
    \end{equation}
      \subitem optimal constant is mean value of target
    \item RMSE := square root of MSE
      \subitem square root is to make units of error same as target
      \subitem every MSE minimizer is a RMSE minimizer and vice versa
      \subitem slight differentce in gradient descent, because
      $$ \frac{\partial RMSE}{\partial \hat{y}_i} = \frac{1}{2\sqrt{MSE}} \frac{\partial MSE}{\partial \hat{y}_i} $$
    \item R-squared
      \begin{equation}
	R^2 := 1 - \frac{MSE}{Var(Y)}
      \end{equation}
      where in this case Var is the biased 1/N type.
      \subitem puts error on reasonable scale: 0 if prediction is no better than optimal constant (negative if worse) and 1 if prediction is perfect
      \subitem to optimize $R^2$, just optimize MSE - just off by two constants
    \item MAE := Mean Absolute Error
      \begin{equation}
        MAE := \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|
      \end{equation}
      \subitem more robust than MSE (less penalty on extremes).
      \subitem widely used in finance (of by \$10 is exactly two times worse than off by \$5)
      \subitem optimal constant is median
      \subitem choice on using MSE vs MAE depends on whether outliers data are bad data, or real data that is just rare.
    \item Problem: 9 vs 10, is same error as 999 vs 1000 in all of the above - this leads to MSPE and MAPE
    \item MSPE (:= mean squared percentage error)
      \begin{equation}
	MSPE = \frac{100}{N} \sum_{i=1}^N\left( \frac{y_i - \hat{y}_i}{y_i} \right)^2
      \end{equation}
    \item MAPE (:= Mean absolute percentage error) 
      \subitem same but with L1 instead of L2.
      \subitem These can be thought of weighted versions of MSE and MAE (with weights not summing to one)
      \subitem optimal constants are weighted versions of mean and median.
    \item RMSLE (:= Root mean square logarithmic error)
      \subitem basically RMSE where you transform $y_i \to \ln(y_i + 1)$ (and same for hat version)
      \subitem this one cares more about relative errors
      \subitem error is assymetric, always better to be above than below
      \subitem best constant is obtained by going to logspace, getting mean, and transforming back
\end{itemize}

\subsection{Classification Metrics}
\begin{itemize}
  \item notation: N is number of objects, L is number of classes, [a = b] is indicator function, 1 if arg is true, 0 otherwise.
  \item Disitinguish between: {\bf hard prediction} - the actual predicted category, vs {\bf soft prediciotns} - the probability for each category
  \item metric 1: Accuray
    \begin{equation}
      Accuracy = \frac{1}{N} \sum_{i=1}^N [\hat{y}_i = y_i]
    \end{equation}
    \subitem uses only hard predictions
    \subitem best constant is most frequent class
    \subitem bad metric with very unbalanced classes, bc best constant gets you really good score
    \subitem difficult to optimize
    \subitem doesn't care about confidence, just right or wrong.
  \item LogLoss
    \subitem for binary (1s, and 0s, specifically)	
    \begin{equation}
      LogLoss = - \frac{1}{N} \sum_{i=1}^N y_i \ln(\hat{y}_i) + (1-y_i)\ln(1-\hat{y}_i)
    \end{equation}	
    \subitem Multiclass generalization
    \begin{equation}
      LogLoss = - \frac{1}{N} \sum_{i=1}^N \sum_{l=1}^L y_{il} \ln(\hat{y}_{il})
    \end{equation}	
    \subitem in practice predictions clipped to avoid nans: min(max($\hat{y}, 10^{-15}), 1-10^{-15})$
    \subitem Logloss penalizes severe mistakes!
    \subitem best constanc for log loss is set $\alpha_i$ (prob class i) to frequency of ith class in target
  \item AUC ROC (Aread under curve - receiver operating curve)
    \subitem only for binary tasks
    \subitem doesn't care about threshold, just ordering of predictions
    \subitem HIS FAVORITE ONE - READ MORE ABOUT THIS!
  \item Cohen's Kappa
    \subitem basically rescales accuracy so that 0 is not 0 accuracy but instead the baseline prediction.
    \subitem read more about this.
\end{itemize}

\section{Metric Optimization}
\begin{itemize}
  \item what we want to optimize is not always what the model optimizes
    \subitem some metrics can be optimized directly by the libraries available: MSE, LogLoss, MAE,  etc
    \subitem sometimes can use an equivalent loss: for example MSPE, RMSLE, can optimize MSE instead if done correctly	
    \subitem someitmes have to use a different loss, and simply postprocess predictions and apply heuristics to adjust for target metric
  \item One method that always works: {\bf early stopping}
    \subitem train using one metric, and validate using another. Then stop training when validation on second metric is lowest.
  \item some tricks:
    \begin{itemize}
      \item often cant optimise MSPE and MAPE out of the box; but you are allowed to pass sample weights. So you can recreate it.
      \item another method for above is to resample dataset according to said weights 
	\subitem leave test set as it. Usually resample many times and average.
      \item for RMSLE just transform to log space, optimize MSE and then transforom predictions back
      \item for classification - you often have to callibrate prediction; basically you fit another model to transform scale predictions appropriately (something simple like a linear model)
      \item Sometimes just tune threshold using grid search
    \end{itemize}
\end{itemize}

\section{extra readings}
\begin{itemize}
  \item All about Learning to Rank problem and some libraries that deal with it
  \item read more when needed.
\end{itemize}

\section{Mean encoding}
\begin{itemize}
  \item main concept: use target to generate new features
    \subitem This often leads to more separable features, which leads to trees needing less depth to reach similar predicitive ability
    \subitem the more non-linear the target dependency on the feature, the more effective mean encoding can be.
    \subitem TIP: If increasing tree depth makes both in sample and out sample better, this is tell-tale sign that mean encoding is probably useful (some features probably have a tremendous number of important split points)
  \item aka likelihood encoding aka target encoding
  \item simplest case: given a categorical variable, encode each level of that variable using the correspoinding target mean for that level
  \item many pitfalls when it comes to doing this - can lead to leakage and overfitting
    \subitem VIP: always train encoding on train data only!
  \item approaches:
    \begin{itemize}
      \item likelihood = (ones/(ones + zeros)) = mean(target)
      \item weighted evidence = $ \ln(ones/zeros) * 100$
      \item count = ones = sum(target)
      \item diff = ones - zeros
    \end{itemize}
  \item regularization for encodings - 4 main approaches
    \begin{itemize}
      \item cross validation loop (robust)
	\subitem split into k-folds (usually 4 or 5) and estimate encoding for values in each fold using only complement.
	\subitem {\color{red} How you train this in train, and then use in validation/test?} Do you average the maps from the folds? Do you treat it as a new fold?
	\subitem Careful - in extreme example, folds = num data, can end up with perfect leak of data
      \item Smoothing
      $$ \frac{mean(target)*nrows + alpha*globalmean}{nrows + alpha} $$
      where nrows is nrows for that given category value, globalmean is the mean of target accross all category values, and alpha is a fixed constant that determines the strength of the smoothing.
      \subitem This punishes rare category encoding.
    \item Noise - just add random noise to target values before doing straight encoding
      \subitem degrades quality of encoding on train data.
      \subitem pretty unstable; must be tunes very carefully
    \item Expanding Mean
      \subitem sort data. Then use only rows from 0 to n-1 to calculate encoding for row n
      \subitem {\color{red} How do we sort the data?} - straightforward when time series, but what if not?
      \subitem this one introduces the least amount of leakage and requires no hyperparameter tuning - it is his favorite.
    \end{itemize}
  \item Extensions and generalizations
    \begin{itemize}
      \item in binary classification, mean is the only relevant statistic. For regression can use things like median, percentiels, std, bins, etc.
      \item in multiclass classification, every feature with lead to L different encodings, one per class.
	\subitem models often solve mutliclass in one-vs-all approach, so these encodings are a good way to give model additional info about other classes
      \item many to many relations - one approach is to stack data, encode results, and then unstack and assign vector value to original, then convert that vector to a useful stat like mean, or std.
	\subitem example: col1 - USERID, col2 - list of apps on phone, target - 0 or 1. Approach is to stack data, have one col for each User/app pair. Encode Apps, and then stack again - will lead to a vector of encoded values (apps) per user.
      \item time series
	\subitem can create very useful and complicate features - i.e. rolling statistics of target variables
	\subitem example: for a given category, calculate mean from previous day, two days, previous week, etc.
	\subitem also, can aggregate along different features. example: col1 Day, col2 user, col3, expense category, col4 amount spent - can do per user average of prev day spending; can also do per category spending for previous day, etc.
	\subitem VIP: don't use future data
      \item numerical features
	\subitem for numerical features can bin and treat as categorical
	\subitem one approach to binning is to fit tree on raw data, and encode based on tree splits
	\subitem if a feature has many splits, it is a good idea to bin (probably using splits of tree)
      \item Interactions
	\subitem simialrly fit tree on raw
	\subitem in resulting trees, look at neighboring splits (i.e. plot tree, and look at all neighboring nodes). The pairs that appear most often, probably have meaningful interaction.
	\subitem then simply concatenate and encode as usual
	\subitem LOOK INTO cat\_boost
    \end{itemize}
\end{itemize}

\section{From Programming Excercise}
\begin{itemize}
    \item cool code: transform
      \begin{codesnip}{\brown}
	all\_data['item\_target\_encode'] = all\_data.groupby('item\_id')['target'].transform('mean')
      \end{codesnip}
      transform returns data frame with index like original df (using the mapping created by the aggregation)
    \item For competition: \underline{Expanding mean scheme} - led to highest correlation of feat with target
    \item For competition: preprocessing approach - for each month, they get all shops and all items, and create the cross product - and fill out with zeros any pair that doesn't have data
  \end{itemize}	


\chapter{Week 4}
\section{Hyperparameter tuning Part I}
\begin{itemize}
  \item VIP: \underline{Do not spend too much time tuning hyperparameters} - features, hacks, leaks, give much bigger boosts
  \item Usually faster to do manual tuning - see results, make smart changes - than automatic tuning
  \item Broadly speaking, can separate parameters into those that increasing parm leads to underfitting (constrain model power) and those that increasing leads to overfitting (increase model fitting power)
  \item GBDT - increase power
  \begin{itemize}
    \item max\_depth (usual range 2-30, typically start around 7)
    \item subsample (see also colsample\_bytree and colsmaple\_bylevel) - number of smapels to use when building each tree
    \item eta (learning rate) (usual range 0.01 to 0.1)
      \subitem if too high, model doesn't converge, if too low takes too long to learn
    \item num\_boost\_round
  \end{itemize}
\item GBDT - regularizing parms
  \begin{itemize}
    \item min\_child\_weight - (wide range: 0, 5, 15, 300, etc) - akin to minimum number of instances that must be in each node (but more elaborate)
      \subitem {\bf One of the most important parameters!}
    \item lambda? alpha?
  \end{itemize}
\item RandomForrest - increase power
  \begin{itemize}
    \item max\_depth  
      \subitem usually optimal is higher than XGBoost	- 10, 20, or higher.
    \item max\_features - num feature to use in tree
  \end{itemize}
\item RF - regularizers
  \begin{itemize}
    \item min\_samples\_leaf
  \end{itemize}
\item RF - other
  \begin{itemize}
    \item N\_estimators - number of trees 
      \subitem this is actually just good to increase - it both fits better and regularizes better
      \subitem start with something small like 10, then go to something large like 300, and try to interpolate to find out what is sufficient
    \item n\_jobs - number of cores
      \subitem {\bf VIP: this defaults to 1 - so change it!}		
  \end{itemize}
\item Neural Network    
  \subitem increase power: num of neurons, number of layers, batch size
  \subitem regularization: L2, L1 for weights, dropout, drop connect, static drop connect (interesting and very powerful)
\item Linear Models
  \begin{itemize}
    \item carefully tuned lightgbm proably beats SVM even on large data sets, but SVM do not require almost any tunning, which is very useful!
    \item sklearn has SVC (SVM classifier) and SVR (SVM regressor) - these wrap liblinear and libsvm
      \subitem must compile yourself for multi-core support
    \item sklean also has logistic regression, linear regression + regularization, SGDClassifier/SGDRegressor	
    \item for data that doesn't fit in memory, use Vowpal Wwabbit
    \item regularization params: C, alpha, L1, L2,\dots
      \subitem for SVM, start with very small C, like $10^{-16}$, and then increase by powers of 10
  \end{itemize}	
\item Other idea: average everything: choose some small spread in parameters, run and average
  \subitem do this for different random initializations too - HELPS ALOT!
\end{itemize}

\section{Random Forrest Notebook Code}
\begin{itemize}
  \item {\brown rf = RandomForrestClassifier(n\_estimators = 500, max\_depth=4, n\_jobs=-1)}
  \item {\brown rf.fit(Xtrain, Ytrain)}
  \item get predictors for each tree
    \begin{codesnip}{\brown}
      predictors = [] \\
      for tree in rf.estimators\_: \\
      \t predictors.append(tree.predict\_proba(Xval)[None,:] \\
	
	predictions = np.vstack(predictions)  \# tensor size of (num trees, num objs, num classes)
    \end{codesnip}
  \item cumulative average
    \begin{codesnip}{\brown}
      cum\_mean = np.cumsum(predictions, axis=0)/np.arange(1, predictions.shape[0]+1)[:, None, None]
    \end{codesnip}
  \item get accuracy
    \begin{codesnip}{\brown}
      scores = [] \\
      for pred in cum\_mean: \\
      \t scores.append(accuracy\_score(Yval, np.argmax(pred, axis=1)))	\\
      plt.plot(scores, linewidth=3)			
    \end{codesnip}
\end{itemize}

\section{Extra Readings}
Sklearn Tuning Hyperparms
\begin{itemize}
  \item use: {\brown estimator.get\_params()} - to see params
  \item two generic sampling approaches: GridSearchCV (all combinations of params) and RandomizedSearchCV (sample parms from specified distribution)
  \item These searches also allow nested estimators, like pipelines and voting classifiers.
  \item VIP: There are some estimators that can efficiently fit some hyperparameters (usually regularizers) at the same time as fitting normal parameters.
    \subitem ex: linear\_model.ElasticNetCV, linear\_model.LogisticRegressionCV
  \item models that use bagging can estimate generalization error on what is left out -- {\bf out-of-bag error}
\end{itemize}
Gradient Boosting Hyperparameters :
\begin{itemize}
  \item {\bf boosting} - ensembles a set of weak learners. At any instance t the model outcomes are weighed based on the ouytcomes of the previous instant t-1. Outcomes predicted correctly are given a lower weight, and the ones missclassified are weighted higher.
  \item bagging controls high variance; boosting controls both bias and variance
  \item General approach for tuning:
    \begin{itemize}
      \item start with relatively high learning rate (say around 0.1) - and determine optimum number of trees for this learning rate (one that works but is still fast, say 40-70)
      \item Tune tree speficic parameters
	\begin{itemize}
	  \item min\_samples\_split - should be about 0.5-1\% of total values  - if imbabalanced data, go with the smaller value
	  \item min\_samples\_leaf - selected based on intution, used to prevent overfitting
	  \item max\_depth - typically 5-8 (for 100k rows and 49 cols he chose 8 - I should probably choose more)
	  \item max\_features - 'sqrt' - it is a general rule of thumb to start with the sqrt of number of features
	  \item subsample  - 0.8 is good starting value
	\end{itemize}
      \item then reduce learning rate and increase trees by same factor. Do a couple of times. (all the way up to something like 2k trees)
    \end{itemize}
  \item he uses GridSerachCV
  \item a good feature importance graph: use many of the variables, none with too much emphasis
  \item XGboost advantages over sklearn.ensemble.GradientBoostingClassifier
    \begin{itemize}
      \item has regularization
      \item much faster (parallel processing)
      \item high flexibility - can customize optimization objectives and evaluation criteria
      \item auto handles missing values
      \item uses tree pruning
      \item built in cross val
      \item can continue from existing model
    \end{itemize}
\end{itemize}	


\section{Tips And Tricks}
\begin{itemize}
  \item Alex Guschin
    \begin{itemize}
      \item KEY: Come up with an idea (ie a feature, a model, etc) try it, and then analyze why your idea works or doesn't work. It will lead to newer/better/more refined ideas.
      \item All parameters are optimizable: From number of features, to constant added at the end of prediction. Sort by 1) importance, 2) feasibility and understanding.
      \subitem Important to consider the downstream effects from a parameter change. They are not all independent! 
  \end{itemize}
\item Dmitry Altukhov
  \begin{itemize}
    \item Preprocess your data and store smartly - has huge effect on time!
      \subitem convert csv/txt to hd5/npy
      \subitem include basic preprocessing like label encoding and joining additional data
      \subitem \underline{ downcast everythin from 64 bit array to 32 bit array  - save memory and time}
    \item Don't start with a complicated cross validation scheme - use holdout at first
    \item use early stopping! its great.
    \item Initial model: lightgbm
      \subitem find reasonably good parameters
      \subitem evaluate features
    \item Switch to tuning models, ensembling and stacking only when satisfied with feature engineering!
    \item fast and dirty is always better: don't code too much. Hard to change.
      \subitem MY COMMENT: {there must be a way to code plenty but not get stuck}
  \end{itemize}
\item Mikhail Tofimov
  \begin{itemize}
    \item Start witha primitive solution to debug full pipeline - from reading data to creating submission
    \item Start with RF instead of GBDT - faster and less tuning
    \item Keep good software practices
      \begin{itemize}
	\item use good variable names
	\item keep research reproducible (fix seeds, comment, use VCS)
	\item reuse your code! - features should be prepared and transformed by the same code during train and test (hard to debug failure to do this)
      \end{itemize}
    \item read scientific articles realated to your competition
  \end{itemize}
\item Dmitry Ulyanov
  \begin{itemize}
    \item Monitor forums and kernels
    \item start with quick EDA
    \item check for leakages
    \item come up with validation process that matches LB score
    \item add features in bulk, put into XGBoost and analyse
    \item when tuning, achieve overfit first and then dial back
    \item Keep code clean
    \item VIP: \underline{\bf restart notebooks} long execution history leads to many global vars, which leads to bugs.
    \item clean code
    \item use git
    \item create separate notebook for each submission - use them as scripts (i.e. restart kernel and run from top to bottom)
    \item use macros!
    \item create a custom library with frequently used functions and classes
  \end{itemize}
\end{itemize}

\section{Competition Pipeline - Mario Michailidis}
\begin{itemize}
  \item Day 1: understand problem, days 2-3: EDA, day 4: define CV strategy, Days 5-(n-4): feature engineering and model building, Day n-3: explore kernels, last 3 days: ensembling and stacking
  \item VIP: ignore outside world for a while - only read other peoples approaches towards the end.
  \item has different anaconda environment for each competition
  \item EDA
    \begin{itemize}
      \item compare training  vs test distribution for each variable
      \item plot feature vs target and vs time (if avaialbe) for each feature
      \item create crosstabs ??
      \item bin numerical features
      \item try to find strange correlations and  non-linearities related to target variable
    \end{itemize}
  \item CV is extremely important (repeats what has already been said above)
  \item Feature Engineering
    \begin{itemize}
      \item Image classification: Scaling, shifting, rotation, CNNs
      \item Sound classification: Fourier, MFCC, specgrams?, scaling
      \item Text Classification: TF-idf, SVD, stemming, speel check, stop word removal, x-grams
      \item Time Series: Lags, weighted averagin, exponential smoothing (see walmart recruitment)
      \item Categorical: Target enc, frequency, one-hot encode, orginal, label encoding (see Amazon employee)
      \item Numerical: scaling, binning, derivatives, outlier removals, dimensionality reduction
      \item Interactions: Multiplications, divisions, group-by factors, concatenations
      \item Recomenders: features based on transactional history, item popularities, frequencos of purchas
    \end{itemize}
  \item VIP: This process can be automated, using selection with cross validation
  \item Modeling
    \begin{itemize}
      \item Image Classification: CNN (Resnet, VGG, denseNet)
      \item Sound Classification: CNN, (CRNN), LSTM
      \item Text Classification: GBMs, Linear, DL, Naiver Bayes, KNNs, LibFM, LibFFM
      \item Time Series: Autoregressive Models, ARIMA, linear, GBMs, DL, LSTMS
      \item \dots 
    \end{itemize}
  \item First tune each individual model, then ensemble
  \item Ensembing
    \begin{itemize}
      \item Different ways to combine, from averaging to stacking
      \item smaller data requires simplers ensembling techniques (like averaging)
      \item helps to average a few low-correlated (a la pearson) predictions with good scores
      \item bigger data can use stacking - stacking basically repeats the modeling process, using model outputs as inputs for next stage
	\subitem can be automated
    \end{itemize}
  \item Tips on collaboration
    \begin{itemize}
      \item It makes it way more fun
      \item you learn more
      \item you score better (cover more ground, get diverse perspectives)
      \item work spearately for a while after CV has been setup
      \item start with people around your rank
    \end{itemize}	
  \item Final tips (for kaggle)
    \begin{itemize}
      \item you never really lose. You always gain knowledge, experience, etc
      \item imagine it is a game (like an RPG)
      \item kaggle community is the most helpfull community
      \item after competition, read other peoples successful approaches
      \item create notebook with useful methods, and update
    \end{itemize}
\end{itemize}

\section{Notebook Macros}
\begin{itemize}
  \item to crea a macro
    \begin{itemize}
      \item put some code you want to store in a cell and run it
      \item {\brown \%macro -q $<$name$>$ $<$cellNumber$>$}
      \item {\brown \% store $<$name$>$} -- sotres in jupyter's global memory
      \item restsrt kernel, load with: {\brown \%store -r $<$name$>$}
    \end{itemize}
  \item can set auto restore in .ipython-profile (must create it)
  \subitem   {\brown !echo ``c = get\_config()$\backslash\backslash${}nc.StoreMagics.autorestore=True'' \\ $>$ $\sim$/.ipython/profile\_default/ipython-config.py}
\end{itemize}

\section{Extra Reading - Jupyter Shortcuts}
\begin{itemize}
  \item Jupyter by default prints only last line in cell; can change to print every var/statement in its one line
    \subitem {\brown  From Ipython.core.interactiveshell import InteractiveShell; InteractiveShell.ast\_node\_interactivity = ''all''}
    \subitem To have this be default: in $\sim/.ipython/profile-default/ipython-config.py$ write
    \subitem {\brown c = get\_config}
    \subitem {\brown c.InteractiveShell.ast\_node\_interactivity = ''all''}
  \item Usefull magics
    \begin{itemize}
      \item \%run
      \item \%load - replace cell with content of external script
      \item \%store - allows passing vars between notebooks (\%sore mydata; del mydata; \%store -r mydata (in other notebook))
      \item \%who - lists all vars in global scope
      \item \%writefile - saves content of a cell to external file
	\subitem \%\%writefile pythoncode.py (to save contents of current cell)
      \item \%pycat pythoncode.py - puts concent of file in \dots ?
      \item \$prun some\_func() - profiler
      \item \%pdv - interface with python debugger
      \item run code from different kernel by starting cell with \%\%$<$kernel$>$ - options: bash, HTML, python2, python3, R, \dots
      \item multi cursor support (like submlime) - hold alt
    \end{itemize}
\end{itemize}

\section{Advanced Feature Engineering II}
\begin{itemize}
  \item Statistics and distance based features
    \begin{itemize}
      \item Basic idea is to not treat each data point indepentely
      \item take a feature, groupby another and compute statistics (can group by several other categories)
      \item If no categories to groupby, can use nearest neighbors
      \item most common example comes from property pricing: use neighbors in the geographical sense - compute average price, number in neighborhood, etc.
      \item howeber, can still apply neihbor concept in abstract space
	\subitem useful distance calculation - {\bf bray-curtis metric}
	$$ \sum \frac{|u_i - v_i|}{|u_i + v_i|} $$
      \item his approach: mean of 5, 10, 15, 500, 2000 neaighbors; also mean distance to 10 closest; mean distance to 10 closest with target 1; etc.
    \end{itemize}
  \item Matrix Factorization for feature extraction
    \begin{itemize}
      \item typical example: movie recomendation.
	\subitem have data set $R$  be users $\times$ movie, and values are ratings (sparse)
	\subitem assume can decompose $R$ into matrix product of $U$ which is users $\times$ some features, and $M$ wich is those features $\times$ Movies.
	\subitem after imposing some restrictions on this decomposition, can the find an optimal one according to some objective (see SVD)
	\subitem some of these features can sometimes be interpreted, like movie sadness, etc, but generally these are abstract.
      \item this is a dimensional reduction technique; but can be used simply to get new representation.
      \item it is a lossy transformation
      \item in sklearn: SVD, PCA, TruncatedSVD (good with sparse matrices), Non-negative Matrix Factorization (NMF) (very useful for decision trees, because ends p very boxy)
      \item these are similar to linear models, so same tricks apply. i.e first log tx the data, etc.
      \item VIP: Must use same transformation on all parts of the data.
	\subitem wrong way:
	\begin{codesnip}{\brown}
	  pca = PCA(5) \\
	  xTrainPCA = pca.fit\_transform(xTrain)\\
	  xTestPCA = pca.fit\_transform(xTest)\\
	\end{codesnip}
	\subitem right way
	\begin{codesnip}{\brown}
	  xAll = np.concatenate([xTrain, xTest]) \\
	  pca.fit(xAll) \\
	  xTrainPCA = pca.transform(xTrain) \\
	  xTestPCA = pca.transform(xTest) 
	\end{codesnip}
	{\color{red} I really need to think about this. this is clearly peeking into the future, to come up with the transformation. But the not peeking way seems worse because the transformations would be different and variables would mean different things!}
    \end{itemize}
  \item Feature Interaction
    \begin{itemize}
      \item approach 1 -  just concatenate values
      \item approach 2 - first apply OHE to both independently, then multiply matrices [multiply each col from first encoding, by each col of second]
      \item both results are practically the same
      \item can apply to real values features too, just multiply them
      \item tree based methods have a hard time reproducing these, so it is good to include them in feature space
      \item to moderate: 1) do feature seletion (prefered) 2) do dimensional reduction
      \item His approach: first to all pairwise sums, fit rf, get most important. Then do all pairwose muls, fit rf, get most important, then prod, then div. Collect and join with original features.
      \item Extract higher orded interactions into categorical features from decision trees:
	\begin{itemize}
	  \item from what I gather, fit a (to target variable) and create a categorical variable mapping each leaf of the tree into a binary feature.
	  \item {\color{red} This seems to me as basically writing the answer down in a categorical variable\dots?}
	  \item in sklearn, {\brown tree\_model.apply()} - takes input feature matrix and returns corresponding index (??) target of leaf
	  \item in xgboost, {\brown booster.predict(pred\_leaf = True)}
	\end{itemize}
    \end{itemize}
  \item t-SNE
    \begin{itemize}
      \item non-linear method of dimensionality reduction - tries to project points from high dimension into a low dim space in such a way that the distances between points are approximately preserved.
      \item widely used in exploratory data analysis, but \underline{very sensitive} to hyperparameters
        \subitem can also be used to obtain new features
      \item main parameter is {\bf perplexity}
      \item in practice:
	\subitem transform train and test together
	\subitem time consuming - if $>500$ features, probably best to do dim rec first
      \item sklearn has implementation, but \underline{much more efficient one in a python package called tsne}
    \end{itemize}
\end{itemize}

\section{Extra readings}
\begin{itemize}
  \item feature transfomration with ensemble of tree
    \begin{itemize}
      \item fit a bunch of trees (random trees, random forrest, GBDT, etc.)
      \item each leaf of each tree assigned a feature index {\color{red} (so each tree is a new feature?)}
      \item pass a sample through each tree, and give it a 1 for leaf (in each tree) it falls into. {\color{red} (So basically OHE each tree-feature?)}
    \end{itemize}
  \item sklearn
    \begin{itemize}
      \item PCA centers, but does not scale before applying SVD
      \item PCA only supports batch processing - for bigger data use IncrementalPCA
    \end{itemize}
  \item Factor Analysis
    \begin{itemize}
      \item data  $X = \{x_1, x_2,\dots, x_n\}$
      \item A very simple continuous latent variable model is
	$$ x_i = Wh_i + \mu + \epsilon $$
	where the $h_i$ are the unobserver/latent variable, $\mu$ is an offset vector and $\epsilon \sim N(0,\Psi)$ is noise.
	\subitem can also be written as $X  = WH + M + E$ (that is, we decomposed X)
	\subitem can also write: $P(x_i | h_j) = N(Wh_j + \mu, \Psi)$
	\subitem {\color{red} I think notation is bad in the main equation above - $x_i = \sum_j W_{ij}h_j +  \dots
	\mu_i? + \epsilon_i? \dots$}
      \item Usual additional assumptions
	\begin{itemize}
	  \item $\Psi = \sigma^2 I$ $\rightarrow$ leads to probabilistic PCA
	  \item $\Psi = diag(\psi_1, \psi_2, \dots, \psi_n)$ $\rightarrow$ leads to factor analysis
	\end{itemize}
      \item main advantage of factor analysis over PCA is that it can model variance in every direction of the input space independently
    \end{itemize}
\end{itemize}

\section{Ensembling}
\begin{itemize}
  \item  Ensemble modelling := combine different ML models to get a better result.
  \item Can simply average (simple, or weighted)
  \item Bagging: averageing slightly different versions of the same model (random forest does this by definiion)
    \subitem reduces error due to variance (doesn't help with bias)
    \subitem he always uses bagging, for any paramter in the model (even seed) - every individual model used later in the ensembling is already a bagged model
  \item Boosting := form of weighted averaging models where each model is built sequentially via taking into account past model performance.
  \item two main types: 1) weight based, 2) Residual error
  \begin{itemize}
    \item fit, predict, generat new weight column: 1 + abs error (many other ways to do this); fit new model using same features and target vars, but including new weight
    \item $pred N = pred0 * \eta + pred1 * \eta + \dots$  {\color{red}(should it not be increasing powers of $\eta$?)}
      \item Rule of thumb: Typically start with a fixed number of estimators, say 100, and then find the best learning rate using CV - then more or less keep their product fixed.
    \end{itemize}
  \item Residual based boosting has been incredibly dominant.
    \begin{itemize}
      \item fit model, predict, and calculate error (not abolute {\color{red}(??)})
      \item then let that error be the new target variable
      \item Final pred = pred0 + pred1 + \dots
    \end{itemize}
  \item favorit implementations: Xgboost, LightGBM (lightning fast), H2O's GBM, catboost, sklearn GMB (can use any sklearn estimator as base)
\end{itemize}

\section{Stacking}
\begin{itemize}
  \item Stacking:= Making predictions of a number of models in a hold-out-set and then using a diferent meta model to train on these predictions.
    \subitem don't use input data for meta model anymore, but then modeling proceeds as usual
  \item VIP: in time series, stacking must respect time.
  \item VIP: Diversity is as important as performance - can even use weak models, so long as they bring something new.
    \subitem get diversity from: different algorithms, different features, different feature transdormations
  \item Meta model is usually very modest - simpler; lower depth, less trees, etc.
  \item He (Marios) talks about his project StackNet
    \subitem uses neural network style architecture - but can't really use backprob b/c not all differentiable, so uses ``stacking'' (?) to link each layer
\end{itemize}

\section{Ensembling Tips and Tricks}
\begin{itemize}
  \item Diversity based on algo
    \subitem 2-2 GBDM, 2-3 NN, 1-2 extra trees, 1-2 linear models, 1-2 KNN models, 1 factorization maching, 1 SVM with non linear kernel
  \item diversity based on input data
    \subitem categorical feats: OHE, label encode, taret encode
    \subitem numerical features: yes or no outlier removal, bin, derivatives, percentiles, scaling
    \subitem interactions: +-*/, groupby stats, unsupervised learning (k-means, SVD, PCS) 
  \item subseqquent level: tree depth of 2 or 3, or NN with 1 or 2 layers
  \item feature engineering
    \subitem create pairwise differences between meta features
    \subitem row-wise statistics
    \subitem employ standards feature selection techniques
  \item rule of thumb - for every 7.5 models in previous level, add 1 in meta
  \item VIP: be very mindful of target leakage
  \item Oweh Zhang: Careful with leakage when stacking - model one will be given alot of weight. Solution, k-fold style data split.

\end{itemize}	

\chapter{Project Issues}
\begin{itemize}
  \item I was unable to get the prediction strength necesary to pass the assignment. Perhaps I will come back to it some months down the line when I am better at this.
  \item Problems (real or supposed)
    \begin{itemize}
      \item I never did any real ensembling - however, I saw other people submit solutions that were plenty good without ensembling
      \item I ended up with a relatively complicated CV scheme, where I had a generate monthly data function, that I called in my rolling cross validator, stacked several months and then fit. This might have been a good idea, but it was difficult to work with, trouble shoot and change as the need arose. It was perhaps not the best strategy.
	\subitem I think I ended up doing this, because I pivoted data to (shop,item) cross month - this way I could do rolling stats, ewms, etc. very easily. But I think in the end, it made my CV much more complicated than other people who simply kep it all stacked and just split at a given row.
	\subitem This also made it difficult so systematically tune hyperparamters -  I should have made it simpler, and used the pipelines and tunning methodologies available.
      \item This pivoting also introduced alot of NaN data, that was not there to begin with, which then took epiciycles to remove - DONT PIVOT if data is not (nearly) complete
      \item I ignored the Russian names (at first), but there was considerable data in these, city, type, etc. Perhaps I should have worked harder here.
      \item At first I didn't add relative change indicators - compare an items price to its rolling mean, normalized by that mean (or a shops revenue, etc.). These, and diffs of these were very important. I believe I did not explore this space enough. \underline{Why were these so important; and distinct from lagged stats?}
      \item I ignored some things that others called outliers - this seems like it was a considerable mistake
    \end{itemize}
  \item In the end, I never figured out what was the biggest problem - I was unable to fix my work. I evaluated others work, but they were mostly variants on a the same theme, a notebook posted in kaggle. While I tried to implement some of the things I saw in these, I must have still missed something.
\end{itemize}	
\end{document}
