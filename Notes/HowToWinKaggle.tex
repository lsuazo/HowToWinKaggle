\documentclass[a4paper]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{xcolor}

\newcommand{\brown}{\color{brown}}
\renewcommand{\t}{\hspace*{0.5cm}}


\newenvironment{tightcenter}{
  \setlength\topsep{0pt}
  \setlength\parskip{0pt}
  \begin{center}
  }{
  \end{center}
}

\newenvironment{codesnip}[1]
{\begin{tightcenter}\begin{minipage}{.85\textwidth}#1}
{\end{minipage}\end{tightcenter}}

\title{How to Win a Data Science Competition: Lean From Top Kagglers}
\author{Coursera - National Research University Higher School of Economics}
\date{2020}

\begin{document}
\maketitle

\chapter{week1}
\section{Introduction and Recap}
\begin{itemize}
  \item In Kaggle competitions, looks for {\bf Kernels} and/or {\bf Notebooks} - people share code that way and you can learn from them.
  \item IMPORTANT: \underline{Insight is more important than algorithm}
  \item IMPORTANT: Don't limit yourself - use advanced feature enginnering; run huge greedy calculations over night.
  \item Recap of models/methods:
    \begin{itemize}
      \item Linear Methods
	\begin{itemize}
	  \item Logistic Regression; Support Vector Machine
	  \item GOOD FOR: Sparse, high dimensional data
	  \item BAD: Often linear separability is not a good approximation
	\end{itemize}
      \item Tree Based Methods
	\begin{itemize}
	  \item Decision Tree; Random Forrest; Gradient Boosted Decision Trees (GBDT)
	  \item GOOD FOR: good default method for tabular data; good for non-linear relationships
	  \item BAD FOR: hard to capture linear separation (say diagonal line in a 2D plane)
	    \subitem from sklearn: can create over-complex trees that do not generalise data well - that is they are prone to overfitting
	    \subitem trees can't extrapolate trends - they are good at interpolation not extrapolation
	  \item \emph{Sklearn} has good random forrest; \emph{xgboost} has good GBDT
	\end{itemize}
      \item K-Nearest neighboors (KNN)
	\subitem good for feature building
      \item Neural Networks (NN)
	\subitem good for images, sounds, texts and sequences
	\subitem very good framework is \emph{pytorch}
    \end{itemize}
  \item Most powerfull methods currently are GBDS and NN
  \item \underline{No free lunch theorem} - there is no single methods that beats all other methods for all applications
  \item TODO: Look into H20 - open source, in-memory, distributed, fast and scalable machine learning and predictive analytics platform.
    \subitem Looks quite interesting!
  \item Hardware: SSD is critical; use cloud computing for anything too big.
\end{itemize}	

\section{Feature Preprocessing}
\begin{itemize}
  \item VIP: Strong connection between preprocessing and model choice
  \item Scaling: $ x_i \to \alpha x_i$
    \begin{itemize}
      \item decision trees are not affected by a scaling tx, but non-tree based methods like NN, kNN and regularized linear models are very affected
	\subitem in KNN - $x \to 0*x$ means ignore $x$, while $x \to \infty x$ means x dominates
	\subitem Any methods that relies on gradient descent is sensitive to scaling (logistic regression, SVM, neural networks, etc. etc)
      \item appraoch 1: Normalization (map to [0,1])
	$$ x \to (x - x.min())/(x.max() - x.min) $$
        \subitem code: {\color{brown} sklearn.preprocessing.MinMaxSaler}
      \item approach 2: standardization (to $\mu=0, \sigma=1$)
	$$ x \to (x - x.mean())/x.std()$$
	\subitem code: {\brown sklearn.preprocessing.StandardScaler} 
      \item Can use scaling to boost or attenuate signals - so scaling can be thought as just another hyperparameter to tune
	\subitem In general, start with all features in the same scale and explore changing that.
    \end{itemize}
  \item Outliers
    \begin{itemize}
      \item linear models extremely sensitive
      \item approach 1: {\bf Winsorization} - clip values between percentiles
	\subitem code: {\brown UPPER, LOWER = np.percentile(x, [1,99]); y = np.clip(x, UPPER, LOWER)}
	\subitem very common in finance
      \item approach 3: Rank
	\subitem good for knn and linear models when you don't have time to handle outliers by hand
      \item approach 3: Other transformation
	\subitem log transformation; raising to power < 1 - these have the benefit of bringing outliers in closer, and spreading things around zero apart
    \end{itemize}
  \item Missing values
    \begin{itemize}
      \item can be Nans, empty strings, outliers like -1, or -999
      \item identifiying when a value actually represents a missing value can be challenging. Main tool: Histogram.
      \item Sometimes missing values contain alot of useful info - there might be a reason the value is missing
	\subitem So it is good to create new features like 'isMissing' or something like that.
      \item 3 main imputation techniques
	\begin{itemize}
	  \item NaN $\to$ value outside range
	    \subitem Gives trees possibilities to use this; Performance for linear models suffers greatly
	    \subitem Curated data often comes with somethign like this already (as described above)
	  \item NaN $\to$ mean, median or mode.
	    \subitem good for linear methods; not good for trees
	  \item Try to reconstruct NaN
	    \subitem not easy; need to know something about the data generating process.
	\end{itemize}
      \item WARNING: \underline{be vary careful with early NaN imputation if you then build features based on imputed feature}
	\subitem for exampl if you fill a cyclic feature with median values, and then construct diff as new feature - will lead to prominent discontinuities and flat zones.
      \item some libraries, like XGBoost can handle NaNs automatically

    \end{itemize}
  \item For all all of these, you must learn the transformation from training data, and then apply the same one on testing. Sklearn Transformation API (used by the transformations above) allows you to do this. VERY GOOD.
  \item USEFUL TRICK: create different predictors from same feature using different preprocessing techniques. 
  \item USEFUL TRICK: Mix models that are trained on different preprocessed versions of the data
\end{itemize}

\section{External reading}
\begin{itemize}
  \item FROM SKLEARN
    \begin{itemize}
      \item when data is sparse, you don't want a scaler that moves zeros. Think about nature of your data to chose scalar.
      \item Transfromer API: {\brown scaler = sklearn.Preprocessing.StandardScaler().fit(X\_train); x\_test\_scaled = scaler(X\_test)}
      \item many estimators in sklearn expect features to look more or less $\sim$ N(0,1)
      \item VIP: Note l1 and l2 regularizes assume all features centered around zero and have variance in the same order.
      \item other useful tx:
	\begin{itemize}
	  \item preprocessing.Normalizer() - scaling individual samples to have unit norm (useful when estimating pair similarity via dot prod)
	  \item preprocessing.OneHotEncoder(), preprocessing.OrdinalEncoder()
	  \item preprocessing.KBinDiscretizer(n\_bins, encode).fit() - different strategies available
	    \subitem histograms focus on counting features in particular bin, whereas dsicretizers focus on labeling features with bin
	\end{itemize}
    \end{itemize}
  \item Sebastian Raschka
    \begin{itemize}
      \item Tree based classification is probably the only scale invariant algo
      \item VIP: when using PCA, it is better to standardize (mean 1, std 0) than just normalize (map to [0,1]) - scaling affects covariance
	\subitem cool example doing PCA and then bayes classifier on top - compar prediction score.
    \end{itemize}
  \item Jason Brownlee - Discover feature engineering: ``Most algorithms are standard - we spend most of our efforts on feature engineering''
  \item Rahul Agarwal - Best practices in feature engineering
    \begin{itemize}
      \item LogLoss clipping technique: clip prediction prob to [0.05. 0.95]  when using log loss metric - it penalizes heavily being very certain and wrong
      \item Use PCA for rotation, not only dim rec
      \item sometimes add interaction features, A*B, A/B, A+B, A - B
    \end{itemize}
\end{itemize}

\section{Feature Generation}
\begin{itemize}
  \item encodings categorical/ordinal
    \begin{itemize}
      \item label encoding: map categories to 1,2,3,4\ldots
	\subitem good for tree based methods, not good for knn or linear (because it assume order and proportionality in labels - moreover dependence is likely not-linear)
	\subitem code: {\brown sklearn.preprocessing.LabelEncoder()} or {\brown pd.factorize()}
      \item Frequency encoding: map to numerical value representing frquency of category
	\subitem can help trees use less splits
	\subitem this is even sometimes useful in linear models
      \item one-hot encoding: create indicator variable for each category
	\subitem these can be good for linear methods, but in general slow down methods (explosion of features) and might not help (specially trees)
	\subitem though will help tree if target depends on lbael encoded feature in a very non linear way
	\subitem code: {\brown pd.get\_dummies()} or {\brown sklearn.preprocessing.OneHotEncoder()}
    \end{itemize}
  \item Datetime and coordinates
    \begin{itemize}
      \item Very different than simple numerical or categorical because we can interpret their meaning - they have much context
      \item dates and times can lead to two  main types of features
	\begin{itemize}
	  \item moments in a period (ie using periodicity of datetime)
	    \subitem ex: day of week, day of month, month in year, etc.  Or minute value, hour value, etc.
	    \subitem useful to capture repetitive pattern in data
	  \item time since/to event
	    \subitem can be row independent: years since 2000, for all rows (so all rows have same reference)
	    \subitem row dependent: days since last holiday (or till next holiday) - two rows can be referring to different holidays
	\end{itemize}
      \item very useful to diffs between two date columns
      \item once you generate new features, numerical or categorical, preprocess them accordingly
      \item coordinates
	\begin{itemize}
	  \item typically you want to calculate distance to points of interest (nearest hospital, school, etc) 
	  \item very useful to calculate aggregated statistics for objects around an area
	    \subitem ex: \# of flats around a point -> proxy for popularity of area
	    \subitem ex: mean price of flats around a point -> gives sense of price of area.
	\end{itemize}
    \end{itemize}
  \item Collection of tricks
    \begin{itemize}
      \item separte price into integer part and fractional part - can utilize differences in peoples \emph{perception} of a price
      \item Create feature, 'isRoundNumber' - people often use numers like 5 and 10, while robots can use many decimals.
      \item One-hot encode interaction between categorical features [just concatenate strings and OHE result]
	\subitem Not so useful in tree based models, because they can easily approximate this with individual categories.
      \item Sometimes simple multiplication, or division of features makes a huge difference.	
	\subitem linear models can't approximate these, and trees have a very hard time approximating them.
      \item sometimes useful to add slightly rotated coordinate system - particularly when using trees. 
	\subitem ex: if a particular street happens to be a good division, but that street is not algined with coordinates, then tree uses many split to approximate.
	\subitem hard to know what rotation to use a priori - so add several and check effect.
    \end{itemize}
\end{itemize}

\section{Feature Extraction From Texts and Images}
\begin{itemize}
  \item For Text\ldots
  \item Preprocessing: 1) lowercase, 2) lemmatization, 3) stemming, 4) remove stopwords
  \item feature extraction: Bag of words - column per word in corpus, row perd doc, count ocurrences
	\subitem can extend to n-grams, of either words of letters
   \item Post processing: TFiDF (Term Frequency and Inverse document frequency)
   \item OR \ldots
   \item embeddings (word2vec, or others)
     \begin{itemize}
       \item Still use preprocessing
       \item create vector representation of words in text
       \item uses nearby words (unsupervised)
       \item often resulting vector space has interpretable operations
       \item training can take a long time - check for pretrained
     \end{itemize}
   \item BOW and Word2Vec often give very different results - use bott together
   \item for images 
     \begin{itemize}
       \item look for pretained models and do some fine tunning
       \item use image augmentation to increase training samples (crops, rotations, adding noise, etc.)
	 \subitem reduces overfitting
     \end{itemize}
\end{itemize}

\section{Questions}
\begin{itemize}
  \item	in GBM\_drop\_tree notebook - why are raw predictions - the output of the staged\_decision\_function - approaching $\pm 8$? (I assume it has to do with depth 3 choice of trees, but still, shouldn't output be close to $\pm1$ which is actual y-values?
\end{itemize}

\chapter{Week 2}
\section{EDA}
\begin{itemize}
  \item VIP: \underline{\bf EDA is key}
    \subitem Kaggle CEO says two apporaches: 1) is EDA other is 2) deep NN and pass everything. They have different domains in which they work better.
  \item Steps:
    \begin{enumerate}
      \item Get Domain knowledge - google, read wiki, read state of the art (put in the time!)
      \item Check whether values in data set agree with domain knowloedge - var ranges, typos, etc. (systematic errors can be very useful info)
      \item KEY: figure out the generation process used in the data - must try to mimic to set up proper validation scheme.
	\subitem perhaps they did random sample; perhaps they oversampled a class to balance out; some times train and test set are produce very differently. INVESTIGATE
    \end{enumerate}
  \item Exploring Anonimized data
    \begin{itemize}
      \item anonimized data is data which orgranizers intentionally change so as to not reveal some information  (ex: replace words with hash values, or col names with x1, x2 ..)
      \item Things to try:
	\begin{itemize}
	  \item try to decode (very diffiicult, and most of the time, can't do it)
	  \item guess the meaning of the column 
	    \subitem cool example was trying to unstardadize a numerical column, by reverse engineering the mean and std dev, to find out original column was year born
	  \item guess the type: numeric, categorical, etc.   - this is generally doable
	  \item find out how feature relate to each other - find pairwise relationships (scatter plots) or even groups (correlatio plots, etc)
	\end{itemize}
      \item tip: label encode using pandas factorize (encodes based on order of appearance)
	\subitem {\color{brown} for c in train.columns[train.dtypes ==  'object']: x[c] = x[c].factorize()[0]}
      \item \underline{\bf VIP TIP}: fit a random forrest and then use {\color{brown} plt.plot(rf.feature\_importances\_} - this can tell you which features you should work on most.
    \end{itemize}
  \item VIP: \underline{Linear models can eaily extract sums and differences, but tree based methods cannot!!}
\end{itemize}	

\section{visualizations}
\begin{itemize}
  \item EDA is an art, and visualizations are our tools
  \item Plots to explore individual features
  \item VIP: \underline{Never make a conlusion from a single plot!} if you have a hypothesis, try to come up with several different plots that could disprove it!
    \begin{itemize}
      \item {\color{brown} plt.hist}
	\subitem can be misleading - vary bin nums. Also zoom in.
	\subitem if see alot of thinks like 12, 24, 26 - i.e. separated by same amount then generate feature x \% 12 (or whatever the appropriate number)
      \item {\color{brown} plt.plot(x,'.')}
	\subitem convenient not to connect points with line segments - just use dots
	\subitem if horizontal lines appear, then lots of repeated values - IN THIS CASE, CREATE FEATURE THAT COUNTS HOW MANY COLS SAME IN THE GROUP - or feature for more nuanced pattern in group.
	\subitem if vertical patterns, then data is not shuffled - IN THIS CASE ADD ROW INDEX AS FEATURE
      \item {\color{brown} plt.scatter(range(len(x), x, c=y)} - very good for looking for separation in the classes based on that feature.
    \end{itemize}
  \item Explore feature relationships
    \begin{itemize}
      \item {\color{brown} plt.scatter(x1, x1)} - \underline{One of the best tools!}
	\subitem usually color by class label too
	\subitem for regression used heaptmap type colors, or visualize target value as point size
  	\subitem TIP: useuful to overlap colored train data with uncolored test data 
	\subitem scatter plots can lead to finding mathematical relations between features (like $x1 \leq x2$) - use these to generate new features like diff or ratio
	\subitem if small number of features: {\color{brown} pd.scatter\_matrix(df)}
      \item large scale feature similarity: {\color{brown} df.corr()} and {\color{brown} plt.matshow()}
	\subitem instead of corr, try to create matrices like: how many times in one feature greater than the other (this can spot cummulative features), or how many distinct combinations of these two features in data exist (can spot relationships between labels)
	\subitem really  cool algorithm for grouping based on these corr values: {\bf spectral biclustering algorithm} - {\color{red}LOOK INTO THIS!}
      \item {\color{brown} df.mean().plot(style='.') }
	\subitem particularly if you sort the columns based on that statistic - might see groups
      \item AND MANY MORE - BE CREATIVE
    \end{itemize}
  \item if you find groups of related features, it is often good to generate new features like a mean value of the group, etc.
  \item nice code when overlapping values:\\
    \begin{codesnip}{\brown}
    def jitter(data, stdev):\\
    \hspace*{1cm} N  = len(data)\\
    \hspace*{1cm} return data + np.random.rand(N)*stdev\\
	plt.scatter(jitter(x, sigma), jitter(y, sigma), c=y)
      \end{codesnip}
\end{itemize}

\section{Dataset cleaning and things to check}
\begin{itemize}
  \item Find (and discard) features that are constant in train and test
    \subitem {\color{brown} traintest.nunique(axis =1) == 1}
    \subitem if constant in training and non-constant in test, still remove
  \item find and remove duplicate features
    \subitem {\color{brown} traintest.T.drop\_duplicates()}
    \subitem if duplicate categorical but with diff cat names label encode first, using factorize()!!\\
  for f in categorical\_feats:\\
  \hspace*{1cm} train[f] = traintest[f].factorize()\\
  traintest.T.drop\_duplicates()
  \item check for duplicate rows
    \subitem check if duplicate rows have diff targets - if so, understand why!
  \item check if train and test have common rows - if so, why?
  \item check if data is shuffled (plot feature or target vs row index)
    \subitem if not shuffled, high chace leakage exists
  \item Nice code - feature histograms
    \begin{codesnip}{\brown}
      def hist\_it(feat): \\
      \t plt.figure(figsize=(16,4))\\
      \t feat[Y == 0].hist(bins=range(int(feat.min()), int(feat.max()+2)), normed=True, alpha=0.8)
      \t feat[Y == 1].hist(bins=range(int(feat.min()), int(feat.max()+2)), normed=True, alpha=0.8)
      \t plt.ylim((0,1))

    \end{codesnip}
  \item Nice code - compare categoricals
    \begin{codesnip}{\brown}
      train\_enc = pd.DataFrame(index = train.index)\\
      for col in tqdm\_notebook(train.columns):\\
      \t train\_enc[col] = train[col].factorize()[0] \\

      dup\_cols = \{  \}
      for i, c1 in enumerate(tqdm\_notebook(train\_enc.columns)):\\
      \t for c2 in train\_enc.columns[i+1: ]:\\
      \t\t if c2 not in dup\_cols and np.all(train\_enc[c1] ==  train\_enc[c2]):
      \t\t\t dup\_cols[c2] == c1
    \end{codesnip}
  \item Usually convenient to concatenate train and test, and do all feature engineering with result (but not always)
\end{itemize}

\section{Validation}
\begin{itemize}
  \item Validation is a piece of test data not used for fitting, but rather checking the value of the model over unseen data. 
  \item Overffiting in general != Overfitting in competitions
    \subitem former: train quality > test quality
    \subitem latter: when quality in test is worse than expected
  \item MAIN POINT: have train/validation split mimic the train/test split - THIS IS KEY. 
    \subitem without this, you validation score does not represent your out of sample test score.
    \subitem Sometimes it can be very challenging to figure out how train/test split was made. It is worth spending considerable time here if needed.
  \item validation strategies
    \begin{itemize}
      \item  3 main: Holdout, K-fold, and leave one out.
      \item holdout - usually a good choice when there is enough data
	\subitem no overlap between train and validation
      \item k-fold - basically repeated holdout, where entire data is partitioned into k validation sets - for each validation set, model is trained on complement. Final measure of performance is average over k folds.
	\subitem core idea: every sample is used for validation exactly once
	\subitem usually k=5 is a good starting point
      \item LOO - is k-fold where k = len(train)
	\subitem used only when there is very little data
    \end{itemize}
  \item usually houldout and k-fold on shuffled data not good when:
    \begin{itemize}
      \item unbalanced data sets (as far as classes)
      \item multiclass classification with many classes
    \end{itemize}
  \item in these cases use {\bf stratification} - preserve the same target distribution in the different folds
  \item these methods are also generally not good for time series data
    \begin{itemize}
      \item you generally need to make a time split - so if holdout, don't shuffle.
	\subitem this emulates how time series data is received, and used
      \item the time series version of k-fold, is a \emph{moving window cross validation} - this rely on capturing trends
      \item OTOH, if for competiotn, test/train split did not use a time split, that means you have future data. Then USE it! And have your cv emulate (this is generally not the case)
      \item models/features that rely on trends tend to be very different than those that rely on future data, so it is key to get this right.
    \end{itemize}
  \item main types of splits used in competitions
    \begin{itemize}
      \item random split (rowwise) - done when rows are fairly independent of each other
      \item time based split
      \item by ID - (typically several rows per ID)
	\subitem test will have IDs not seen in train
      \item combined - ex: date split for each ID independently
      \item non-trivial
    \end{itemize}
  \item Typical problems encountered during validation
    \begin{itemize}
      \item Different folds lead to very different values
	\subitem can happen when different folds are very different in nature: example, in competition, cross-validating with january vs february can be very different (number of holidays is very different) {\color{red} HINT HINT}
	\subitem can also be caused by too little data, or data that is too diverse, or data that is inconsistent (i.e. similar samples with very different target values - error changes if they are both in train, or one and one)
      \item {\bf leaderboard shuffle} - very different public/ private scores.
      \item some solutions: increase k. Or redo k-fold with different seeds (can use one to get parameters and one to test)
    \end{itemize}
  \item submission stage problems: LB consistently higher (lower) than cv; LB uncorrelated with CV 
    \subitem can be caused by train/test data comminb from diff distributions -- Best friend EDA: problem is typically that you haven't mimicked split correctly
    \subitem Other trick: adjust solution based on LB - find optimal constant to add/subtract from your preductions based on public leaderboard score
    \subitem most often problem comes from imbalanced classes. solution, try to mimic the split.
    \subitem can also be casued by too little public test data; in this case just trust you CV
\end{itemize}

\section{Data leakages}
\begin{itemize}
  \item very bad, people get very sensitive, to exploit or not exploit? (shouldn't and often can't in real world)
    \subitem When they exist, they tend to dominate
  \item typical leaks
    \begin{itemize}
      \item future peaking [incorrect splits]
      \item metadata
      \item IDS sometimes contain information
      \item row ordering
      \item LB probing (can be used to extract what they call \emph{ground truth}
    \end{itemize}
\end{itemize}

\section{Additional Material And Links}
\begin{itemize}
  \item CV in sklearn
    \begin{itemize}
      \item Validation is a way to fit hyperparameters of model without burning data
      \item KEY: Preprocessing should also be learned from training, so it is good to use a {\bf pipeline} to combine prepreocessing with validation
	\subitem ex:
	\begin{codesnip}{\brown}
	  from sklearn.pipeline import make\_pipeline\\
	  clf = make\_pipeline(preprocessing.standardScaler(), svm.svc(c=1))\\
	  cross\_val\_score(clf, X, y, cv=cv)
	\end{codesnip}
      \item random split: {\brown sklearn.model\_selection.train\_test\_split}
      \item k-fold: {\brown sklearn.model\_selection.cross\_val\_score(df, x, y, cv=5, scoring='\ldots')}
	\subitem can pass a cross-validation scheme to cv to have more control, or even a custom iterable yielding (test, train) splits
	\subitem more flexible variant: cross\_validate
      \item many cross validation iterators that can be used, for iid and not iid (grouped, imbalanced classes, time series)
    \end{itemize}
  \item Some dude's lessons
    \begin{itemize}
      \item always cv (not just v)
      \item trust good CV method more than LB
      \item for final submission picke two very different models (ie. from bagging of SVM, vs Random forrest, vs neural network, vs linear models)
    \end{itemize}
  \item Kaggle CEO lessons learned
    \begin{itemize}
      \item Use sklearn pipelines to avoid leakages, peaking, etc.
      \item in 2011 random forrest won alot; in 2012 deep NN started dominating (at least vision and time series); in 2015 XGboost started dominating
      \item Two main approaches to dominate
	\begin{itemize}
	  \item XGBoost with serious EDA
	  \item deep NN with very little EDA
	\end{itemize}
      \item Top Kaggle participant attributes
	\begin{itemize}
	  \item creativity - create lots and lots of features
	  \item Tenacity - keep working, not get despirited
	  \item very good with statistics [avoid overfitting]
	  \item good software practices [like VC]
	\end{itemize}
      \item suggestion: look at kaggle public data sets, and use their script forking
    \end{itemize}
\end{itemize}


\end{document}
