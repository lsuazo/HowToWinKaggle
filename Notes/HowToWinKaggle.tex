\documentclass[a4paper]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{xcolor}
\usepackage{amsmath}		

\newcommand{\brown}{\color{brown}}
\renewcommand{\t}{\hspace*{0.5cm}}


\newenvironment{tightcenter}{
  \setlength\topsep{0pt}
  \setlength\parskip{0pt}
  \begin{center}
  }{
  \end{center}
}

\newenvironment{codesnip}[1]
{\begin{tightcenter}\begin{minipage}{.85\textwidth}#1}
{\end{minipage}\end{tightcenter}}

\title{How to Win a Data Science Competition: Lean From Top Kagglers}
\author{Coursera - National Research University Higher School of Economics}
\date{2020}

\begin{document}
\maketitle

\chapter{week1}
\section{Introduction and Recap}
\begin{itemize}
  \item In Kaggle competitions, looks for {\bf Kernels} and/or {\bf Notebooks} - people share code that way and you can learn from them.
  \item IMPORTANT: \underline{Insight is more important than algorithm}
  \item IMPORTANT: Don't limit yourself - use advanced feature enginnering; run huge greedy calculations over night.
  \item Recap of models/methods:
    \begin{itemize}
      \item Linear Methods
	\begin{itemize}
	  \item Logistic Regression; Support Vector Machine
	  \item GOOD FOR: Sparse, high dimensional data
	  \item BAD: Often linear separability is not a good approximation
	\end{itemize}
      \item Tree Based Methods
	\begin{itemize}
	  \item Decision Tree; Random Forrest; Gradient Boosted Decision Trees (GBDT)
	  \item GOOD FOR: good default method for tabular data; good for non-linear relationships
	  \item BAD FOR: hard to capture linear separation (say diagonal line in a 2D plane)
	    \subitem from sklearn: can create over-complex trees that do not generalise data well - that is they are prone to overfitting
	    \subitem trees can't extrapolate trends - they are good at interpolation not extrapolation
	  \item \emph{Sklearn} has good random forrest; \emph{xgboost} has good GBDT
	\end{itemize}
      \item K-Nearest neighboors (KNN)
	\subitem good for feature building
      \item Neural Networks (NN)
	\subitem good for images, sounds, texts and sequences
	\subitem very good framework is \emph{pytorch}
    \end{itemize}
  \item Most powerfull methods currently are GBDS and NN
  \item \underline{No free lunch theorem} - there is no single methods that beats all other methods for all applications
  \item TODO: Look into H20 - open source, in-memory, distributed, fast and scalable machine learning and predictive analytics platform.
    \subitem Looks quite interesting!
  \item Hardware: SSD is critical; use cloud computing for anything too big.
\end{itemize}	

\section{Feature Preprocessing}
\begin{itemize}
  \item VIP: Strong connection between preprocessing and model choice
  \item Scaling: $ x_i \to \alpha x_i$
    \begin{itemize}
      \item decision trees are not affected by a scaling tx, but non-tree based methods like NN, kNN and regularized linear models are very affected
	\subitem in KNN - $x \to 0*x$ means ignore $x$, while $x \to \infty x$ means x dominates
	\subitem Any methods that relies on gradient descent is sensitive to scaling (logistic regression, SVM, neural networks, etc. etc)
      \item appraoch 1: Normalization (map to [0,1])
	$$ x \to (x - x.min())/(x.max() - x.min) $$
        \subitem code: {\color{brown} sklearn.preprocessing.MinMaxSaler}
      \item approach 2: standardization (to $\mu=0, \sigma=1$)
	$$ x \to (x - x.mean())/x.std()$$
	\subitem code: {\brown sklearn.preprocessing.StandardScaler} 
      \item Can use scaling to boost or attenuate signals - so scaling can be thought as just another hyperparameter to tune
	\subitem In general, start with all features in the same scale and explore changing that.
    \end{itemize}
  \item Outliers
    \begin{itemize}
      \item linear models extremely sensitive
      \item approach 1: {\bf Winsorization} - clip values between percentiles
	\subitem code: {\brown UPPER, LOWER = np.percentile(x, [1,99]); y = np.clip(x, UPPER, LOWER)}
	\subitem very common in finance
      \item approach 3: Rank
	\subitem good for knn and linear models when you don't have time to handle outliers by hand
      \item approach 3: Other transformation
	\subitem log transformation; raising to power < 1 - these have the benefit of bringing outliers in closer, and spreading things around zero apart
    \end{itemize}
  \item Missing values
    \begin{itemize}
      \item can be Nans, empty strings, outliers like -1, or -999
      \item identifiying when a value actually represents a missing value can be challenging. Main tool: Histogram.
      \item Sometimes missing values contain alot of useful info - there might be a reason the value is missing
	\subitem So it is good to create new features like 'isMissing' or something like that.
      \item 3 main imputation techniques
	\begin{itemize}
	  \item NaN $\to$ value outside range
	    \subitem Gives trees possibilities to use this; Performance for linear models suffers greatly
	    \subitem Curated data often comes with somethign like this already (as described above)
	  \item NaN $\to$ mean, median or mode.
	    \subitem good for linear methods; not good for trees
	  \item Try to reconstruct NaN
	    \subitem not easy; need to know something about the data generating process.
	\end{itemize}
      \item WARNING: \underline{be vary careful with early NaN imputation if you then build features based on imputed feature}
	\subitem for exampl if you fill a cyclic feature with median values, and then construct diff as new feature - will lead to prominent discontinuities and flat zones.
      \item some libraries, like XGBoost can handle NaNs automatically

    \end{itemize}
  \item For all all of these, you must learn the transformation from training data, and then apply the same one on testing. Sklearn Transformation API (used by the transformations above) allows you to do this. VERY GOOD.
  \item USEFUL TRICK: create different predictors from same feature using different preprocessing techniques. 
  \item USEFUL TRICK: Mix models that are trained on different preprocessed versions of the data
\end{itemize}

\section{External reading}
\begin{itemize}
  \item FROM SKLEARN
    \begin{itemize}
      \item when data is sparse, you don't want a scaler that moves zeros. Think about nature of your data to chose scalar.
      \item Transfromer API: {\brown scaler = sklearn.Preprocessing.StandardScaler().fit(X\_train); x\_test\_scaled = scaler(X\_test)}
      \item many estimators in sklearn expect features to look more or less $\sim$ N(0,1)
      \item VIP: Note l1 and l2 regularizes assume all features centered around zero and have variance in the same order.
      \item other useful tx:
	\begin{itemize}
	  \item preprocessing.Normalizer() - scaling individual samples to have unit norm (useful when estimating pair similarity via dot prod)
	  \item preprocessing.OneHotEncoder(), preprocessing.OrdinalEncoder()
	  \item preprocessing.KBinDiscretizer(n\_bins, encode).fit() - different strategies available
	    \subitem histograms focus on counting features in particular bin, whereas dsicretizers focus on labeling features with bin
	\end{itemize}
    \end{itemize}
  \item Sebastian Raschka
    \begin{itemize}
      \item Tree based classification is probably the only scale invariant algo
      \item VIP: when using PCA, it is better to standardize (mean 1, std 0) than just normalize (map to [0,1]) - scaling affects covariance
	\subitem cool example doing PCA and then bayes classifier on top - compar prediction score.
    \end{itemize}
  \item Jason Brownlee - Discover feature engineering: ``Most algorithms are standard - we spend most of our efforts on feature engineering''
  \item Rahul Agarwal - Best practices in feature engineering
    \begin{itemize}
      \item LogLoss clipping technique: clip prediction prob to [0.05. 0.95]  when using log loss metric - it penalizes heavily being very certain and wrong
      \item Use PCA for rotation, not only dim rec
      \item sometimes add interaction features, A*B, A/B, A+B, A - B
    \end{itemize}
\end{itemize}

\section{Feature Generation}
\begin{itemize}
  \item encodings categorical/ordinal
    \begin{itemize}
      \item label encoding: map categories to 1,2,3,4\ldots
	\subitem good for tree based methods, not good for knn or linear (because it assume order and proportionality in labels - moreover dependence is likely not-linear)
	\subitem code: {\brown sklearn.preprocessing.LabelEncoder()} or {\brown pd.factorize()}
      \item Frequency encoding: map to numerical value representing frquency of category
	\subitem can help trees use less splits
	\subitem this is even sometimes useful in linear models
      \item one-hot encoding: create indicator variable for each category
	\subitem these can be good for linear methods, but in general slow down methods (explosion of features) and might not help (specially trees)
	\subitem though will help tree if target depends on lbael encoded feature in a very non linear way
	\subitem code: {\brown pd.get\_dummies()} or {\brown sklearn.preprocessing.OneHotEncoder()}
    \end{itemize}
  \item Datetime and coordinates
    \begin{itemize}
      \item Very different than simple numerical or categorical because we can interpret their meaning - they have much context
      \item dates and times can lead to two  main types of features
	\begin{itemize}
	  \item moments in a period (ie using periodicity of datetime)
	    \subitem ex: day of week, day of month, month in year, etc.  Or minute value, hour value, etc.
	    \subitem useful to capture repetitive pattern in data
	  \item time since/to event
	    \subitem can be row independent: years since 2000, for all rows (so all rows have same reference)
	    \subitem row dependent: days since last holiday (or till next holiday) - two rows can be referring to different holidays
	\end{itemize}
      \item very useful to diffs between two date columns
      \item once you generate new features, numerical or categorical, preprocess them accordingly
      \item coordinates
	\begin{itemize}
	  \item typically you want to calculate distance to points of interest (nearest hospital, school, etc) 
	  \item very useful to calculate aggregated statistics for objects around an area
	    \subitem ex: \# of flats around a point -> proxy for popularity of area
	    \subitem ex: mean price of flats around a point -> gives sense of price of area.
	\end{itemize}
    \end{itemize}
  \item Collection of tricks
    \begin{itemize}
      \item separte price into integer part and fractional part - can utilize differences in peoples \emph{perception} of a price
      \item Create feature, 'isRoundNumber' - people often use numers like 5 and 10, while robots can use many decimals.
      \item One-hot encode interaction between categorical features [just concatenate strings and OHE result]
	\subitem Not so useful in tree based models, because they can easily approximate this with individual categories.
      \item Sometimes simple multiplication, or division of features makes a huge difference.	
	\subitem linear models can't approximate these, and trees have a very hard time approximating them.
      \item sometimes useful to add slightly rotated coordinate system - particularly when using trees. 
	\subitem ex: if a particular street happens to be a good division, but that street is not algined with coordinates, then tree uses many split to approximate.
	\subitem hard to know what rotation to use a priori - so add several and check effect.
    \end{itemize}
\end{itemize}

\section{Feature Extraction From Texts and Images}
\begin{itemize}
  \item For Text\ldots
  \item Preprocessing: 1) lowercase, 2) lemmatization, 3) stemming, 4) remove stopwords
  \item feature extraction: Bag of words - column per word in corpus, row perd doc, count ocurrences
	\subitem can extend to n-grams, of either words of letters
   \item Post processing: TFiDF (Term Frequency and Inverse document frequency)
   \item OR \ldots
   \item embeddings (word2vec, or others)
     \begin{itemize}
       \item Still use preprocessing
       \item create vector representation of words in text
       \item uses nearby words (unsupervised)
       \item often resulting vector space has interpretable operations
       \item training can take a long time - check for pretrained
     \end{itemize}
   \item BOW and Word2Vec often give very different results - use bott together
   \item for images 
     \begin{itemize}
       \item look for pretained models and do some fine tunning
       \item use image augmentation to increase training samples (crops, rotations, adding noise, etc.)
	 \subitem reduces overfitting
     \end{itemize}
\end{itemize}

\section{Questions}
\begin{itemize}
  \item	in GBM\_drop\_tree notebook - why are raw predictions - the output of the staged\_decision\_function - approaching $\pm 8$? (I assume it has to do with depth 3 choice of trees, but still, shouldn't output be close to $\pm1$ which is actual y-values?
\end{itemize}

\chapter{Week 2}
\section{EDA}
\begin{itemize}
  \item VIP: \underline{\bf EDA is key}
    \subitem Kaggle CEO says two apporaches: 1) is EDA other is 2) deep NN and pass everything. They have different domains in which they work better.
  \item Steps:
    \begin{enumerate}
      \item Get Domain knowledge - google, read wiki, read state of the art (put in the time!)
      \item Check whether values in data set agree with domain knowloedge - var ranges, typos, etc. (systematic errors can be very useful info)
      \item KEY: figure out the generation process used in the data - must try to mimic to set up proper validation scheme.
	\subitem perhaps they did random sample; perhaps they oversampled a class to balance out; some times train and test set are produce very differently. INVESTIGATE
    \end{enumerate}
  \item Exploring Anonimized data
    \begin{itemize}
      \item anonimized data is data which orgranizers intentionally change so as to not reveal some information  (ex: replace words with hash values, or col names with x1, x2 ..)
      \item Things to try:
	\begin{itemize}
	  \item try to decode (very diffiicult, and most of the time, can't do it)
	  \item guess the meaning of the column 
	    \subitem cool example was trying to unstardadize a numerical column, by reverse engineering the mean and std dev, to find out original column was year born
	  \item guess the type: numeric, categorical, etc.   - this is generally doable
	  \item find out how feature relate to each other - find pairwise relationships (scatter plots) or even groups (correlatio plots, etc)
	\end{itemize}
      \item tip: label encode using pandas factorize (encodes based on order of appearance)
	\subitem {\color{brown} for c in train.columns[train.dtypes ==  'object']: x[c] = x[c].factorize()[0]}
      \item \underline{\bf VIP TIP}: fit a random forrest and then use {\color{brown} plt.plot(rf.feature\_importances\_} - this can tell you which features you should work on most.
    \end{itemize}
  \item VIP: \underline{Linear models can eaily extract sums and differences, but tree based methods cannot!!}
\end{itemize}	

\section{visualizations}
\begin{itemize}
  \item EDA is an art, and visualizations are our tools
  \item Plots to explore individual features
  \item VIP: \underline{Never make a conlusion from a single plot!} if you have a hypothesis, try to come up with several different plots that could disprove it!
    \begin{itemize}
      \item {\color{brown} plt.hist}
	\subitem can be misleading - vary bin nums. Also zoom in.
	\subitem if see alot of thinks like 12, 24, 26 - i.e. separated by same amount then generate feature x \% 12 (or whatever the appropriate number)
      \item {\color{brown} plt.plot(x,'.')}
	\subitem convenient not to connect points with line segments - just use dots
	\subitem if horizontal lines appear, then lots of repeated values - IN THIS CASE, CREATE FEATURE THAT COUNTS HOW MANY COLS SAME IN THE GROUP - or feature for more nuanced pattern in group.
	\subitem if vertical patterns, then data is not shuffled - IN THIS CASE ADD ROW INDEX AS FEATURE
      \item {\color{brown} plt.scatter(range(len(x), x, c=y)} - very good for looking for separation in the classes based on that feature.
    \end{itemize}
  \item Explore feature relationships
    \begin{itemize}
      \item {\color{brown} plt.scatter(x1, x1)} - \underline{One of the best tools!}
	\subitem usually color by class label too
	\subitem for regression used heaptmap type colors, or visualize target value as point size
  	\subitem TIP: useuful to overlap colored train data with uncolored test data 
	\subitem scatter plots can lead to finding mathematical relations between features (like $x1 \leq x2$) - use these to generate new features like diff or ratio
	\subitem if small number of features: {\color{brown} pd.scatter\_matrix(df)}
      \item large scale feature similarity: {\color{brown} df.corr()} and {\color{brown} plt.matshow()}
	\subitem instead of corr, try to create matrices like: how many times in one feature greater than the other (this can spot cummulative features), or how many distinct combinations of these two features in data exist (can spot relationships between labels)
	\subitem really  cool algorithm for grouping based on these corr values: {\bf spectral biclustering algorithm} - {\color{red}LOOK INTO THIS!}
      \item {\color{brown} df.mean().plot(style='.') }
	\subitem particularly if you sort the columns based on that statistic - might see groups
      \item AND MANY MORE - BE CREATIVE
    \end{itemize}
  \item if you find groups of related features, it is often good to generate new features like a mean value of the group, etc.
  \item nice code when overlapping values:\\
    \begin{codesnip}{\brown}
    def jitter(data, stdev):\\
    \hspace*{1cm} N  = len(data)\\
    \hspace*{1cm} return data + np.random.rand(N)*stdev\\
	plt.scatter(jitter(x, sigma), jitter(y, sigma), c=y)
      \end{codesnip}
\end{itemize}

\section{Dataset cleaning and things to check}
\begin{itemize}
  \item Find (and discard) features that are constant in train and test
    \subitem {\color{brown} traintest.nunique(axis =1) == 1}
    \subitem if constant in training and non-constant in test, still remove
  \item find and remove duplicate features
    \subitem {\color{brown} traintest.T.drop\_duplicates()}
    \subitem if duplicate categorical but with diff cat names label encode first, using factorize()!!\\
  for f in categorical\_feats:\\
  \hspace*{1cm} train[f] = traintest[f].factorize()\\
  traintest.T.drop\_duplicates()
  \item check for duplicate rows
    \subitem check if duplicate rows have diff targets - if so, understand why!
  \item check if train and test have common rows - if so, why?
  \item check if data is shuffled (plot feature or target vs row index)
    \subitem if not shuffled, high chace leakage exists
  \item Nice code - feature histograms
    \begin{codesnip}{\brown}
      def hist\_it(feat): \\
      \t plt.figure(figsize=(16,4))\\
      \t feat[Y == 0].hist(bins=range(int(feat.min()), int(feat.max()+2)), normed=True, alpha=0.8)
      \t feat[Y == 1].hist(bins=range(int(feat.min()), int(feat.max()+2)), normed=True, alpha=0.8)
      \t plt.ylim((0,1))

    \end{codesnip}
  \item Nice code - compare categoricals
    \begin{codesnip}{\brown}
      train\_enc = pd.DataFrame(index = train.index)\\
      for col in tqdm\_notebook(train.columns):\\
      \t train\_enc[col] = train[col].factorize()[0] \\

      dup\_cols = \{  \}
      for i, c1 in enumerate(tqdm\_notebook(train\_enc.columns)):\\
      \t for c2 in train\_enc.columns[i+1: ]:\\
      \t\t if c2 not in dup\_cols and np.all(train\_enc[c1] ==  train\_enc[c2]):
      \t\t\t dup\_cols[c2] == c1
    \end{codesnip}
  \item Usually convenient to concatenate train and test, and do all feature engineering with result (but not always)
\end{itemize}

\section{Validation}
\begin{itemize}
  \item Validation is a piece of test data not used for fitting, but rather checking the value of the model over unseen data. 
  \item Overffiting in general != Overfitting in competitions
    \subitem former: train quality > test quality
    \subitem latter: when quality in test is worse than expected
  \item MAIN POINT: have train/validation split mimic the train/test split - THIS IS KEY. 
    \subitem without this, you validation score does not represent your out of sample test score.
    \subitem Sometimes it can be very challenging to figure out how train/test split was made. It is worth spending considerable time here if needed.
  \item validation strategies
    \begin{itemize}
      \item  3 main: Holdout, K-fold, and leave one out.
      \item holdout - usually a good choice when there is enough data
	\subitem no overlap between train and validation
      \item k-fold - basically repeated holdout, where entire data is partitioned into k validation sets - for each validation set, model is trained on complement. Final measure of performance is average over k folds.
	\subitem core idea: every sample is used for validation exactly once
	\subitem usually k=5 is a good starting point
      \item LOO - is k-fold where k = len(train)
	\subitem used only when there is very little data
    \end{itemize}
  \item usually houldout and k-fold on shuffled data not good when:
    \begin{itemize}
      \item unbalanced data sets (as far as classes)
      \item multiclass classification with many classes
    \end{itemize}
  \item in these cases use {\bf stratification} - preserve the same target distribution in the different folds
  \item these methods are also generally not good for time series data
    \begin{itemize}
      \item you generally need to make a time split - so if holdout, don't shuffle.
	\subitem this emulates how time series data is received, and used
      \item the time series version of k-fold, is a \emph{moving window cross validation} - this rely on capturing trends
      \item OTOH, if for competiotn, test/train split did not use a time split, that means you have future data. Then USE it! And have your cv emulate (this is generally not the case)
      \item models/features that rely on trends tend to be very different than those that rely on future data, so it is key to get this right.
    \end{itemize}
  \item main types of splits used in competitions
    \begin{itemize}
      \item random split (rowwise) - done when rows are fairly independent of each other
      \item time based split
      \item by ID - (typically several rows per ID)
	\subitem test will have IDs not seen in train
      \item combined - ex: date split for each ID independently
      \item non-trivial
    \end{itemize}
  \item Typical problems encountered during validation
    \begin{itemize}
      \item Different folds lead to very different values
	\subitem can happen when different folds are very different in nature: example, in competition, cross-validating with january vs february can be very different (number of holidays is very different) {\color{red} HINT HINT}
	\subitem can also be caused by too little data, or data that is too diverse, or data that is inconsistent (i.e. similar samples with very different target values - error changes if they are both in train, or one and one)
      \item {\bf leaderboard shuffle} - very different public/ private scores.
      \item some solutions: increase k. Or redo k-fold with different seeds (can use one to get parameters and one to test)
    \end{itemize}
  \item submission stage problems: LB consistently higher (lower) than cv; LB uncorrelated with CV 
    \subitem can be caused by train/test data comminb from diff distributions -- Best friend EDA: problem is typically that you haven't mimicked split correctly
    \subitem Other trick: adjust solution based on LB - find optimal constant to add/subtract from your preductions based on public leaderboard score
    \subitem most often problem comes from imbalanced classes. solution, try to mimic the split.
    \subitem can also be casued by too little public test data; in this case just trust you CV
\end{itemize}

\section{Data leakages}
\begin{itemize}
  \item very bad, people get very sensitive, to exploit or not exploit? (shouldn't and often can't in real world)
    \subitem When they exist, they tend to dominate
  \item typical leaks
    \begin{itemize}
      \item future peaking [incorrect splits]
      \item metadata
      \item IDS sometimes contain information
      \item row ordering
      \item LB probing (can be used to extract what they call \emph{ground truth}
    \end{itemize}
\end{itemize}

\section{Additional Material And Links}
\begin{itemize}
  \item CV in sklearn
    \begin{itemize}
      \item Validation is a way to fit hyperparameters of model without burning data
      \item KEY: Preprocessing should also be learned from training, so it is good to use a {\bf pipeline} to combine prepreocessing with validation
	\subitem ex:
	\begin{codesnip}{\brown}
	  from sklearn.pipeline import make\_pipeline\\
	  clf = make\_pipeline(preprocessing.standardScaler(), svm.svc(c=1))\\
	  cross\_val\_score(clf, X, y, cv=cv)
	\end{codesnip}
      \item random split: {\brown sklearn.model\_selection.train\_test\_split}
      \item k-fold: {\brown sklearn.model\_selection.cross\_val\_score(df, x, y, cv=5, scoring='\ldots')}
	\subitem can pass a cross-validation scheme to cv to have more control, or even a custom iterable yielding (test, train) splits
	\subitem more flexible variant: cross\_validate
      \item many cross validation iterators that can be used, for iid and not iid (grouped, imbalanced classes, time series)
    \end{itemize}
  \item Some dude's lessons
    \begin{itemize}
      \item always cv (not just v)
      \item trust good CV method more than LB
      \item for final submission picke two very different models (ie. from bagging of SVM, vs Random forrest, vs neural network, vs linear models)
    \end{itemize}
  \item Kaggle CEO lessons learned
    \begin{itemize}
      \item Use sklearn pipelines to avoid leakages, peaking, etc.
      \item in 2011 random forrest won alot; in 2012 deep NN started dominating (at least vision and time series); in 2015 XGboost started dominating
      \item Two main approaches to dominate
	\begin{itemize}
	  \item XGBoost with serious EDA
	  \item deep NN with very little EDA
	\end{itemize}
      \item Top Kaggle participant attributes
	\begin{itemize}
	  \item creativity - create lots and lots of features
	  \item Tenacity - keep working, not get despirited
	  \item very good with statistics [avoid overfitting]
	  \item good software practices [like VC]
	\end{itemize}
      \item suggestion: look at kaggle public data sets, and use their script forking
    \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Week 3}
\section{Evaluation Metrics}
\begin{itemize}
  \item  Problem: Some metrics cannot be optimized efficiently. So need to come up with proxy metric, and find heuristics to transform optimized metric results to final submission.
  \item also, if train and test sets are different, might need to modify optimization metric a bit (happens often in time series, where distribution changes over time)
  \item interesting example metrix (comes from finance)
    \begin{equation}
      Loss(\hat{y}_i, y_i) = 
      \begin{cases}
	|y_i - \hat{y}_i| & \mbox{ if trend predicted correctly} \\
	(y_i - \hat{y}_i)^2 & \mbox{ if trend predicted incorrecly}
      \end{cases}
    \end{equation}
    \subitem punishes one type of error much more.
    \subitem Hard to optimize with an algorithm
    \subitem in his case, he hacked the metric - it was all about making small predictions, and guessing right size (or something like that)
  \item Do exploratory analysis of unusual metrics!
  \item ALWAYS BUILD A BEST CONSTANT BASELINE (see below)
\end{itemize}

\subsection{Regression metrics}
\begin{itemize}
  \item MSE := Mean Squared Error
    \begin{equation}
      MSE = \frac{1}{N}\sum_{i=1}^N(y_i - \hat{y}_i)^2
    \end{equation}
      \subitem optimal constant is mean value of target
    \item RMSE := square root of MSE
      \subitem square root is to make units of error same as target
      \subitem every MSE minimizer is a RMSE minimizer and vice versa
      \subitem slight differentce in gradient descent, because
      $$ \frac{\partial RMSE}{\partial \hat{y}_i} = \frac{1}{2\sqrt{MSE}} \frac{\partial MSE}{\partial \hat{y}_i} $$
    \item R-squared
      \begin{equation}
	R^2 := 1 - \frac{MSE}{Var(Y)}
      \end{equation}
      where in this case Var is the biased 1/N type.
      \subitem puts error on reasonable scale: 0 if prediction is no better than optimal constant (negative if worse) and 1 if prediction is perfect
      \subitem to optimize $R^2$, just optimize MSE - just off by two constants
    \item MAE := Mean Absolute Error
      \begin{equation}
        MAE := \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|
      \end{equation}
      \subitem more robust than MSE (less penalty on extremes).
      \subitem widely used in finance (of by \$10 is exactly two times worse than off by \$5)
      \subitem optimal constant is median
      \subitem choice on using MSE vs MAE depends on whether outliers data are bad data, or real data that is just rare.
    \item Problem: 9 vs 10, is same error as 999 vs 1000 in all of the above - this leads to MSPE and MAPE
    \item MSPE (:= mean squared percentage error)
      \begin{equation}
	MSPE = \frac{100}{N} \sum_{i=1}^N\left( \frac{y_i - \hat{y}_i}{y_i} \right)^2
      \end{equation}
    \item MAPE (:= Mean absolute percentage error) 
      \subitem same but with L1 instead of L2.
      \subitem These can be thought of weighted versions of MSE and MAE (with weights not summing to one)
      \subitem optimal constants are weighted versions of mean and median.
    \item RMSLE (:= Root mean square logarithmic error)
      \subitem basically RMSE where you transform $y_i \to \ln(y_i + 1)$ (and same for hat version)
      \subitem this one cares more about relative errors
      \subitem error is assymetric, always better to be above than below
      \subitem best constant is obtained by going to logspace, getting mean, and transforming back
\end{itemize}

\subsection{Classification Metrics}
\begin{itemize}
  \item notation: N is number of objects, L is number of classes, [a = b] is indicator function, 1 if arg is true, 0 otherwise.
  \item Disitinguish between: {\bf hard prediction} - the actual predicted category, vs {\bf soft prediciotns} - the probability for each category
  \item metric 1: Accuray
    \begin{equation}
      Accuracy = \frac{1}{N} \sum_{i=1}^N [\hat{y}_i = y_i]
    \end{equation}
    \subitem uses only hard predictions
    \subitem best constant is most frequent class
    \subitem bad metric with very unbalanced classes, bc best constant gets you really good score
    \subitem difficult to optimize
    \subitem doesn't care about confidence, just right or wrong.
  \item LogLoss
    \subitem for binary (1s, and 0s, specifically)	
    \begin{equation}
      LogLoss = - \frac{1}{N} \sum_{i=1}^N y_i \ln(\hat{y}_i) + (1-y_i)\ln(1-\hat{y}_i)
    \end{equation}	
    \subitem Multiclass generalization
    \begin{equation}
      LogLoss = - \frac{1}{N} \sum_{i=1}^N \sum_{l=1}^L y_{il} \ln(\hat{y}_{il})
    \end{equation}	
    \subitem in practice predictions clipped to avoid nans: min(max($\hat{y}, 10^{-15}), 1-10^{-15})$
    \subitem Logloss penalizes severe mistakes!
    \subitem best constanc for log loss is set $\alpha_i$ (prob class i) to frequency of ith class in target
  \item AUC ROC (Aread under curve - receiver operating curve)
    \subitem only for binary tasks
    \subitem doesn't care about threshold, just ordering of predictions
    \subitem HIS FAVORITE ONE - READ MORE ABOUT THIS!
  \item Cohen's Kappa
    \subitem basically rescales accuracy so that 0 is not 0 accuracy but instead the baseline prediction.
    \subitem read more about this.
\end{itemize}

\section{Metric Optimization}
\begin{itemize}
  \item what we want to optimize is not always what the model optimizes
    \subitem some metrics can be optimized directly by the libraries available: MSE, LogLoss, MAE,  etc
    \subitem sometimes can use an equivalent loss: for example MSPE, RMSLE, can optimize MSE instead if done correctly	
    \subitem someitmes have to use a different loss, and simply postprocess predictions and apply heuristics to adjust for target metric
  \item One method that always works: {\bf early stopping}
    \subitem train using one metric, and validate using another. Then stop training when validation on second metric is lowest.
  \item some tricks:
    \begin{itemize}
      \item often cant optimise MSPE and MAPE out of the box; but you are allowed to pass sample weights. So you can recreate it.
      \item another method for above is to resample dataset according to said weights 
	\subitem leave test set as it. Usually resample many times and average.
      \item for RMSLE just transform to log space, optimize MSE and then transforom predictions back
      \item for classification - you often have to callibrate prediction; basically you fit another model to transform scale predictions appropriately (something simple like a linear model)
      \item Sometimes just tune threshold using grid search
    \end{itemize}
\end{itemize}

\section{extra readings}
\begin{itemize}
  \item All about Learning to Rank problem and some libraries that deal with it
  \item read more when needed.
\end{itemize}

\section{Mean encoding}
\begin{itemize}
  \item main concept: use target to generate new features
    \subitem This often leads to more separable features, which leads to trees needing less depth to reach similar predicitive ability
    \subitem the more non-linear the target dependency on the feature, the more effective mean encoding can be.
    \subitem TIP: If increasing tree depth makes both in sample and out sample better, this is tell-tale sign that mean encoding is probably useful (some features probably have a tremendous number of important split points)
  \item aka likelihood encoding aka target encoding
  \item simplest case: given a categorical variable, encode each level of that variable using the correspoinding target mean for that level
  \item many pitfalls when it comes to doing this - can lead to leakage and overfitting
    \subitem VIP: always train encoding on train data only!
  \item approaches:
    \begin{itemize}
      \item likelihood = (ones/(ones + zeros)) = mean(target)
      \item weighted evidence = $ \ln(ones/zeros) * 100$
      \item count = ones = sum(target)
      \item diff = ones - zeros
    \end{itemize}
  \item regularization for encodings - 4 main approaches
    \begin{itemize}
      \item cross validation loop (robust)
	\subitem split into k-folds (usually 4 or 5) and estimate encoding for values in each fold using only complement.
	\subitem {\color{red} How you train this in train, and then use in validation/test?} Do you average the maps from the folds? Do you treat it as a new fold?
	\subitem Careful - in extreme example, folds = num data, can end up with perfect leak of data
      \item Smoothing
      $$ \frac{mean(target)*nrows + alpha*globalmean}{nrows + alpha} $$
      where nrows is nrows for that given category value, globalmean is the mean of target accross all category values, and alpha is a fixed constant that determines the strength of the smoothing.
      \subitem This punishes rare category encoding.
    \item Noise - just add random noise to target values before doing straight encoding
      \subitem degrades quality of encoding on train data.
      \subitem pretty unstable; must be tunes very carefully
    \item Expanding Mean
      \subitem sort data. Then use only rows from 0 to n-1 to calculate encoding for row n
      \subitem {\color{red} How do we sort the data?} - straightforward when time series, but what if not?
      \subitem this one introduces the least amount of leakage and requires no hyperparameter tuning - it is his favorite.
    \end{itemize}
  \item Extensions and generalizations
    \begin{itemize}
      \item in binary classification, mean is the only relevant statistic. For regression can use things like median, percentiels, std, bins, etc.
      \item in multiclass classification, every feature with lead to L different encodings, one per class.
	\subitem models often solve mutliclass in one-vs-all approach, so these encodings are a good way to give model additional info about other classes
      \item many to many relations - one approach is to stack data, encode results, and then unstack and assign vector value to original, then convert that vector to a useful stat like mean, or std.
	\subitem example: col1 - USERID, col2 - list of apps on phone, target - 0 or 1. Approach is to stack data, have one col for each User/app pair. Encode Apps, and then stack again - will lead to a vector of encoded values (apps) per user.
      \item time series
	\subitem can create very useful and complicate features - i.e. rolling statistics of target variables
	\subitem example: for a given category, calculate mean from previous day, two days, previous week, etc.
	\subitem also, can aggregate along different features. example: col1 Day, col2 user, col3, expense category, col4 amount spent - can do per user average of prev day spending; can also do per category spending for previous day, etc.
	\subitem VIP: don't use future data
      \item numerical features
	\subitem for numerical features can bin and treat as categorical
	\subitem one approach to binning is to fit tree on raw data, and encode based on tree splits
	\subitem if a feature has many splits, it is a good idea to bin (probably using splits of tree)
      \item Interactions
	\subitem simialrly fit tree on raw
	\subitem in resulting trees, look at neighboring splits (i.e. plot tree, and look at all neighboring nodes). The pairs that appear most often, probably have meaningful interaction.
	\subitem then simply concatenate and encode as usual
	\subitem LOOK INTO cat\_boost
    \end{itemize}
\end{itemize}

\section{From Programming Excercise}
\begin{itemize}
    \item cool code: transform
      \begin{codesnip}{\brown}
	all\_data['item\_target\_encode'] = all\_data.groupby('item\_id')['target'].transform('mean')
      \end{codesnip}
      transform returns data frame with index like original df (using the mapping created by the aggregation)
    \item For competition: \underline{Expanding mean scheme} - led to highest correlation of feat with target
    \item For competition: preprocessing approach - for each month, they get all shops and all items, and create the cross product - and fill out with zeros any pair that doesn't have data
  \end{itemize}	


\chapter{Week 4}
\section{Hyperparameter tuning Part I}
\begin{itemize}
  \item VIP: \underline{Do not spend too much time tuning hyperparameters} - features, hacks, leaks, give much bigger boosts
  \item Usually faster to do manual tuning - see results, make smart changes - than automatic tuning
  \item Broadly speaking, can separate parameters into those that increasing parm leads to underfitting (constrain model power) and those that increasing leads to overfitting (increase model fitting power)
  \item GBDT - increase power
  \begin{itemize}
    \item max\_depth (usual range 2-30, typically start around 7)
    \item subsample (see also colsample\_bytree and colsmaple\_bylevel) - number of smapels to use when building each tree
    \item eta (learning rate) (usual range 0.01 to 0.1)
      \subitem if too high, model doesn't converge, if too low takes too long to learn
    \item num\_boost\_round
  \end{itemize}
\item GBDT - regularizing parms
  \begin{itemize}
    \item min\_child\_weight - (wide range: 0, 5, 15, 300, etc) - akin to minimum number of instances that must be in each node (but more elaborate)
      \subitem {\bf One of the most important parameters!}
    \item lambda? alpha?
  \end{itemize}
\item RandomForrest - increase power
  \begin{itemize}
    \item max\_depth  
      \subitem usually optimal is higher than XGBoost	- 10, 20, or higher.
    \item max\_features - num feature to use in tree
  \end{itemize}
\item RF - regularizers
  \begin{itemize}
    \item min\_samples\_leaf
  \end{itemize}
\item RF - other
  \begin{itemize}
    \item N\_estimators - number of trees 
      \subitem this is actually just good to increase - it both fits better and regularizes better
      \subitem start with something small like 10, then go to something large like 300, and try to interpolate to find out what is sufficient
    \item n\_jobs - number of cores
      \subitem {\bf VIP: this defaults to 1 - so change it!}		
  \end{itemize}
\item Neural Network    
  \subitem increase power: num of neurons, number of layers, batch size
  \subitem regularization: L2, L1 for weights, dropout, drop connect, static drop connect (interesting and very powerful)
\item Linear Models
  \begin{itemize}
    \item carefully tuned lightgbm proably beats SVM even on large data sets, but SVM do not require almost any tunning, which is very useful!
    \item sklearn has SVC (SVM classifier) and SVR (SVM regressor) - these wrap liblinear and libsvm
      \subitem must compile yourself for multi-core support
    \item sklean also has logistic regression, linear regression + regularization, SGDClassifier/SGDRegressor	
    \item for data that doesn't fit in memory, use Vowpal Wwabbit
    \item regularization params: C, alpha, L1, L2,\dots
      \subitem for SVM, start with very small C, like $10^{-16}$, and then increase by powers of 10
  \end{itemize}	
\item Other idea: average everything: choose some small spread in parameters, run and average
  \subitem do this for different random initializations too - HELPS ALOT!
\end{itemize}

\section{Random Forrest Notebook Code}
\begin{itemize}
  \item {\brown rf = RandomForrestClassifier(n\_estimators = 500, max\_depth=4, n\_jobs=-1)}
  \item {\brown rf.fit(Xtrain, Ytrain)}
  \item get predictors for each tree
    \begin{codesnip}{\brown}
      predictors = [] \\
      for tree in rf.estimators\_: \\
      \t predictors.append(tree.predict\_proba(Xval)[None,:] \\
	
	predictions = np.vstack(predictions)  \# tensor size of (num trees, num objs, num classes)
    \end{codesnip}
  \item cumulative average
    \begin{codesnip}{\brown}
      cum\_mean = np.cumsum(predictions, axis=0)/np.arange(1, predictions.shape[0]+1)[:, None, None]
    \end{codesnip}
  \item get accuracy
    \begin{codesnip}{\brown}
      scores = [] \\
      for pred in cum\_mean: \\
      \t scores.append(accuracy\_score(Yval, np.argmax(pred, axis=1)))	\\
      plt.plot(scores, linewidth=3)			
    \end{codesnip}
\end{itemize}

\section{Extra Readings}
Sklearn Tuning Hyperparms
\begin{itemize}
  \item use: {\brown estimator.get\_params()} - to see params
  \item two generic sampling approaches: GridSearchCV (all combinations of params) and RandomizedSearchCV (sample parms from specified distribution)
  \item These searches also allow nested estimators, like pipelines and voting classifiers.
  \item VIP: There are some estimators that can efficiently fit some hyperparameters (usually regularizers) at the same time as fitting normal parameters.
    \subitem ex: linear\_model.ElasticNetCV, linear\_model.LogisticRegressionCV
  \item models that use bagging can estimate generalization error on what is left out -- {\bf out-of-bag error}
\end{itemize}
Gradient Boosting Hyperparameters :
\begin{itemize}
  \item {\bf boosting} - ensembles a set of weak learners. At any instance t the model outcomes are weighed based on the ouytcomes of the previous instant t-1. Outcomes predicted correctly are given a lower weight, and the ones missclassified are weighted higher.
  \item bagging controls high variance; boosting controls both bias and variance
  \item General approach for tuning:
    \begin{itemize}
      \item start with relatively high learning rate (say around 0.1) - and determine optimum number of trees for this learning rate (one that works but is still fast, say 40-70)
      \item Tune tree speficic parameters
	\begin{itemize}
	  \item min\_samples\_split - should be about 0.5-1\% of total values  - if imbabalanced data, go with the smaller value
	  \item min\_samples\_leaf - selected based on intution, used to prevent overfitting
	  \item max\_depth - typically 5-8 (for 100k rows and 49 cols he chose 8 - I should probably choose more)
	  \item max\_features - 'sqrt' - it is a general rule of thumb to start with the sqrt of number of features
	  \item subsample  - 0.8 is good starting value
	\end{itemize}
      \item then reduce learning rate and increase trees by same factor. Do a couple of times. (all the way up to something like 2k trees)
    \end{itemize}
  \item he uses GridSerachCV
  \item a good feature importance graph: use many of the variables, none with too much emphasis
  \item XGboost advantages over sklearn.ensemble.GradientBoostingClassifier
    \begin{itemize}
      \item has regularization
      \item much faster (parallel processing)
      \item high flexibility - can customize optimization objectives and evaluation criteria
      \item auto handles missing values
      \item uses tree pruning
      \item built in cross val
      \item can continue from existing model
    \end{itemize}
\end{itemize}	


\section{Tips And Tricks}
\subsection{Alex Guschin}
\begin{itemize}
  \item
\end{itemize}

\end{document}
