{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always want to optimize for the competition metric\n",
    "- This can lead to a metric that is hard to optimize\n",
    "\n",
    "May also want to consider a different approach if improving comp metric score on validation set does not lead to improved comp score on test set\n",
    "- In particular, time series data can be challenging\n",
    "\n",
    "Do exploratory metric analysis (esp for peculiar metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE\n",
    "minimized by constant vector equal to the mean value of the dataset\n",
    "\n",
    "This is true for MSE raised to any arbitrary positive power, including RMSE (so optimizing one means optimizing the other)\n",
    "Numerics of MSE vs RMSE\n",
    "however, while the derivative vectors point in the same direction, they don't have the same scale.  In practice, one may need to alter the learning rate in an algorithm that optimizes RMSE to get the same value\n",
    "\n",
    "Finally, optimizing MSE is equivalent to optimizing Rsquared and Rsquared is more interpretable/meaningful\n",
    "- When Rsquared = 0, your model score is the same as choosing the optimizing constant vector (along $[1, 1, \\dots, 1]$\n",
    "- When Rsquared < 0, your  model score is worse than choosing the sample average\n",
    "- When Rsquared = 1, your model is perfect\n",
    "\n",
    "MAE\n",
    "for a given sample, loss only grows linearly with y_pred - y (rather than quadratically)\n",
    "\n",
    "The constant vector that optimizes MAE is the median (know that this estimator is less sensitive to outliers, so this is a way of seeing that MAE is less sensitive to outliers.\n",
    "\n",
    "In a real problem, it is a good idea to use MAE over MSE if there are thought to be outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for classification\n",
    "\n",
    "Accuracy\n",
    "\n",
    "\n",
    "LogLoss\n",
    "It is just log of the probability your model assigned to the output.  \n",
    "\n",
    "\n",
    "Fixing accuracy with Cohen's Kappa.  Let's normalize accuracy so that 0 = baseline and 1 = perfect score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping: Have a metric which is easy to optimize efficiently but is not the true metric we care about.  With each iteration of the easy metric, evaluate the true metric, and stop iterating when the true metric stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE is a special case of quantile loss\n",
    "MAE is sometimes called median regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a metric with sample weights, can optimize by resampling your train set with a probability propto the metric weight.  I like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize mean square log loss by transforming variables $z = \\mathrm{log}(1 + y)$ and then using MSE.  I Like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking reminds me of neural nets and multilevel models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
